# 统一API配置文件
api:
  version: "v1"
  host: "0.0.0.0"
  port: 8000
  base_path: "/api/v1"
  
# 引擎配置
engines:
  # OpenAI配置
  openai:
    enabled: true
    endpoint: "https://api.openai.com/v1"
    api_key: "${OPENAI_API_KEY}"
    default_model: "gpt-3.5-turbo"
    rate_limit: 3000
    timeout: 60
    models:
      - "gpt-4"
      - "gpt-4-turbo"
      - "gpt-3.5-turbo"
      - "gpt-3.5-turbo-16k"
  
  # Google Gemini配置
  gemini:
    enabled: true
    api_key: "${GEMINI_API_KEY}"
    default_model: "gemini-1.5-pro"
    generation_config:
      temperature: 0.7
      top_p: 0.8
      top_k: 40
      max_output_tokens: 2048
    models:
      - "gemini-1.5-pro"
      - "gemini-1.5-flash"
      - "gemini-pro"
  
  # Anthropic Claude配置
  claude:
    enabled: false
    endpoint: "https://api.anthropic.com"
    api_key: "${CLAUDE_API_KEY}"
    default_model: "claude-3-sonnet-20240229"
    version: "2023-06-01"
    timeout: 60
    models:
      - "claude-3-opus-20240229"
      - "claude-3-sonnet-20240229"
      - "claude-3-haiku-20240307"
  
  # llama.cpp本地引擎配置
  llamacpp:
    enabled: true
    model_path: "./models"
    default_model: "qwen3:4b"
    context_length: 4096
    gpu_layers: 35
    threads: 8
    batch_size: 512
    models:
      - name: "qwen3:4b"
        path: "qwen3-4b-q4_0.gguf"
        context_length: 4096
      - name: "qwen2.5:7b"
        path: "qwen2.5-7b-q4_0.gguf"
        context_length: 8192
      - name: "deepseek:7b"
        path: "deepseek-coder-7b-q4_0.gguf"
        context_length: 8192
      - name: "llama3:8b"
        path: "llama3-8b-q4_0.gguf"
        context_length: 8192
  
  # Ollama配置
  ollama:
    enabled: true
    endpoint: "http://localhost:11434"
    default_model: "qwen2.5:4b"
    timeout: 60
    models:
      - "qwen2.5:4b"
      - "qwen2.5:7b"
      - "deepseek-coder:7b"
      - "llama3:8b"
      - "llama3.1:8b"
      - "codellama:7b"
  
  # vLLM配置（可选）
  vllm:
    enabled: false
    endpoint: "http://localhost:8000"
    models:
      - "meta-llama/Llama-2-7b-chat-hf"
      - "meta-llama/Llama-2-13b-chat-hf"

# GPU配置
gpu:
  enabled: true
  auto_detect: true
  gpu_layers: 35
  main_gpu: 0
  tensor_split: []
  memory_limit: null
  low_vram: false
  batch_size: 512

# 路由配置
routing:
  strategy: "smart_route"  # local_first, cloud_first, smart_route, cost_optimized, performance_optimized
  complexity_threshold: 0.7
  local_preference: 0.6
  cost_weight: 0.3
  performance_weight: 0.4
  reliability_weight: 0.3
  fallback_strategy: "local_first"
  max_retries: 3
  
# 性能配置
performance:
  connection_pool:
    max_connections: 100
    timeout: 30
  cache:
    enabled: true
    ttl: 3600
    max_size: 1000
  batch_processing:
    max_batch_size: 10
    max_wait_time: 0.1

# 安全配置
security:
  api_keys:
    required: false
    header_name: "Authorization"
  rate_limiting:
    enabled: true
    requests_per_minute: 100
    burst_size: 10

# 日志配置
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/unified_api.log"
  max_size: "10MB"
  backup_count: 5