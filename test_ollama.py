#!/usr/bin/env python3
"""
Ollamaé›†æˆæµ‹è¯•è„šæœ¬
"""
import asyncio
import logging
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'python'))

from core.ollama_client import OllamaClient
from core.grpc_client import GRPCClient
from agent.orchestrator import AgentOrchestrator
from models.schemas import ChatRequest

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def test_ollama_client():
    """æµ‹è¯•Ollamaå®¢æˆ·ç«¯"""
    logger.info("ğŸ§ª æµ‹è¯•Ollamaå®¢æˆ·ç«¯...")
    
    client = OllamaClient()
    
    try:
        await client.initialize()
        logger.info("âœ… Ollamaå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # è·å–å¯ç”¨æ¨¡å‹
        models = await client.get_available_models()
        logger.info(f"âœ… å¯ç”¨æ¨¡å‹: {models}")
        
        # æµ‹è¯•èŠå¤©
        messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}]
        response = await client.chat_completion(messages)
        
        if "error" not in response:
            logger.info(f"âœ… èŠå¤©æµ‹è¯•æˆåŠŸ:")
            logger.info(f"  ğŸ“ å›å¤: {response['content'][:100]}...")
            logger.info(f"  ğŸ§  æ¨¡å‹: {response.get('model')}")
            logger.info(f"  â±ï¸ å»¶è¿Ÿ: {response.get('latency_ms', 0):.1f}ms")
        else:
            logger.error(f"âŒ èŠå¤©æµ‹è¯•å¤±è´¥: {response['error']}")
        
        await client.cleanup()
        return True
        
    except Exception as e:
        logger.error(f"âŒ Ollamaå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_grpc_with_ollama():
    """æµ‹è¯•é›†æˆOllamaçš„gRPCå®¢æˆ·ç«¯"""
    logger.info("ğŸ”§ æµ‹è¯•gRPCå®¢æˆ·ç«¯ï¼ˆé›†æˆOllamaï¼‰...")
    
    grpc_client = GRPCClient()
    
    try:
        await grpc_client.connect()
        logger.info("âœ… gRPCå®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        
        # æµ‹è¯•æ¨ç†
        response = await grpc_client.inference(
            prompt="è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½",
            max_tokens=100
        )
        
        logger.info(f"âœ… æ¨ç†æµ‹è¯•æˆåŠŸ:")
        logger.info(f"  ğŸ“ å›å¤: {response['text'][:100]}...")
        logger.info(f"  ğŸ§  æ¨¡å‹: {response.get('used_model')}")
        logger.info(f"  â±ï¸ å»¶è¿Ÿ: {response.get('latency_ms', 0):.1f}ms")
        
        # è·å–æ¨¡å‹åˆ—è¡¨
        models = await grpc_client.get_available_models()
        logger.info(f"âœ… å¯ç”¨æ¨¡å‹: {models}")
        
        await grpc_client.disconnect()
        return True
        
    except Exception as e:
        logger.error(f"âŒ gRPCå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_agent_with_ollama():
    """æµ‹è¯•é›†æˆOllamaçš„Agentè°ƒåº¦å™¨"""
    logger.info("ğŸ¤– æµ‹è¯•Agentè°ƒåº¦å™¨ï¼ˆé›†æˆOllamaï¼‰...")
    
    grpc_client = GRPCClient()
    orchestrator = AgentOrchestrator(grpc_client)
    
    try:
        await orchestrator.initialize()
        logger.info("âœ… Agentè°ƒåº¦å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        test_cases = [
            "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",  # åº”è¯¥è¢«è¯†åˆ«ä¸ºéœ€è¦æ’ä»¶
            "è¯·è¯¦ç»†åˆ†æä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿",  # å¤æ‚ä»»åŠ¡
        ]
        
        for i, message in enumerate(test_cases, 1):
            logger.info(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i}: {message} ---")
            
            request = ChatRequest(
                message=message,
                session_id=f"test_session_{i}"
            )
            
            response = await orchestrator.process_chat(request)
            
            logger.info(f"âœ… å¯¹è¯{i}æˆåŠŸ:")
            logger.info(f"  ğŸ“ å›å¤: {response.content[:100]}...")
            logger.info(f"  ğŸ§  ä½¿ç”¨æ¨¡å‹: {response.model_used}")
            logger.info(f"  ğŸ’­ å†³ç­–åŸå› : {response.reasoning}")
            logger.info(f"  â±ï¸ å“åº”æ—¶é—´: {response.latency_ms or 0:.1f}ms")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = orchestrator.get_stats()
        logger.info(f"\nâœ… æ€§èƒ½ç»Ÿè®¡: {stats}")
        
        await orchestrator.cleanup()
        return True
        
    except Exception as e:
        logger.error(f"âŒ Agentè°ƒåº¦å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹Ollamaé›†æˆæµ‹è¯•")
    logger.info("=" * 60)
    
    test_functions = [
        ("Ollamaå®¢æˆ·ç«¯", test_ollama_client),
        ("gRPCå®¢æˆ·ç«¯(é›†æˆOllama)", test_grpc_with_ollama),
        ("Agentè°ƒåº¦å™¨(é›†æˆOllama)", test_agent_with_ollama),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in test_functions:
        try:
            logger.info(f"\nğŸ” å¼€å§‹æµ‹è¯•: {test_name}")
            success = await test_func()
            if success:
                passed += 1
                logger.info(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                failed += 1
                logger.error(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            failed += 1
            logger.error(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
        
        logger.info("=" * 60)
    
    logger.info(f"\nğŸ æµ‹è¯•å®Œæˆ")
    logger.info(f"âœ… é€šè¿‡: {passed}")
    logger.info(f"âŒ å¤±è´¥: {failed}")
    logger.info(f"ğŸ“Š æˆåŠŸç‡: {passed/(passed+failed)*100:.1f}%")
    
    if failed == 0:
        logger.info("ğŸ‰ æ‰€æœ‰Ollamaé›†æˆæµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        logger.warning("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€")
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿è¡Œå¼‚å¸¸: {e}")
        sys.exit(1)