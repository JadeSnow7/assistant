#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å¥—ä»¶
éªŒè¯å®Œæ•´ä¸šåŠ¡æµç¨‹ï¼ŒåŒ…æ‹¬æ™ºèƒ½å¯¹è¯ã€ç³»ç»Ÿè¯Šæ–­ã€æ¨èæ‰§è¡Œç­‰åœºæ™¯
"""
import asyncio
import time
from typing import List, Dict, Any, Optional
from tests.base import BaseTestSuite, TestCase, TestResult, TestMetrics


class EndToEndIntegrationTestSuite(BaseTestSuite):
    """ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        super().__init__(base_url)
        self.e2e_test_cases = self._create_e2e_test_cases()
    
    def _create_e2e_test_cases(self) -> List[TestCase]:
        """åˆ›å»ºç«¯åˆ°ç«¯æµ‹è¯•ç”¨ä¾‹"""
        return [
            # åœºæ™¯1ï¼šæ™ºèƒ½ç³»ç»Ÿè¯Šæ–­æµç¨‹
            TestCase(
                name="intelligent_system_diagnosis",
                description="æ™ºèƒ½ç³»ç»Ÿè¯Šæ–­å®Œæ•´æµç¨‹",
                input_data={
                    "scenario": "performance_issue",
                    "user_complaint": "æˆ‘çš„ç”µè„‘æœ€è¿‘å¾ˆå¡ï¼Œå¸®æˆ‘çœ‹çœ‹ä»€ä¹ˆé—®é¢˜",
                    "expected_flow": ["é—®é¢˜è¯†åˆ«", "ç³»ç»Ÿæ£€æŸ¥", "æ¨èæ–¹æ¡ˆ", "æ‰§è¡Œç¡®è®¤"],
                    "max_time": 15000
                },
                expected_output={
                    "diagnosis_provided": True,
                    "recommendations_given": True,
                    "actionable_steps": True,
                    "user_friendly": True
                },
                category="ç³»ç»Ÿè¯Šæ–­æµç¨‹",
                priority="HIGH"
            ),
            
            # åœºæ™¯2ï¼šä¿¡æ¯æŸ¥è¯¢ä¸å¤„ç†æµç¨‹
            TestCase(
                name="information_query_processing",
                description="ä¿¡æ¯æŸ¥è¯¢ä¸å¤„ç†å®Œæ•´æµç¨‹",
                input_data={
                    "scenario": "weather_and_planning",
                    "user_query": "æ˜å¤©åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿé€‚åˆæˆ·å¤–æ´»åŠ¨å—ï¼Ÿ",
                    "expected_flow": ["å¤©æ°”æŸ¥è¯¢", "æ•°æ®åˆ†æ", "å»ºè®®ç”Ÿæˆ"],
                    "max_time": 10000
                },
                expected_output={
                    "weather_data_retrieved": True,
                    "analysis_provided": True,
                    "recommendations_given": True,
                    "contextual_advice": True
                },
                category="ä¿¡æ¯æŸ¥è¯¢æµç¨‹",
                priority="HIGH"
            ),
            
            # åœºæ™¯3ï¼šå¤æ‚ä»»åŠ¡åˆ†è§£ä¸æ‰§è¡Œ
            TestCase(
                name="complex_task_decomposition",
                description="å¤æ‚ä»»åŠ¡åˆ†è§£ä¸æ‰§è¡Œæµç¨‹",
                input_data={
                    "scenario": "project_planning",
                    "user_request": "å¸®æˆ‘åˆ¶å®šä¸€ä¸ªå­¦ä¹ Pythonçš„è®¡åˆ’ï¼ŒåŒ…æ‹¬æ—¶é—´å®‰æ’å’Œèµ„æºæ¨è",
                    "expected_flow": ["éœ€æ±‚åˆ†æ", "ä»»åŠ¡åˆ†è§£", "èµ„æºåŒ¹é…", "è®¡åˆ’ç”Ÿæˆ"],
                    "max_time": 20000
                },
                expected_output={
                    "comprehensive_plan": True,
                    "time_structured": True,
                    "resource_links": True,
                    "personalized": True
                },
                category="ä»»åŠ¡è§„åˆ’æµç¨‹",
                priority="MEDIUM"
            ),
            
            # åœºæ™¯4ï¼šå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ä¿æŒ
            TestCase(
                name="multi_turn_context_flow",
                description="å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ä¿æŒæµç¨‹",
                input_data={
                    "scenario": "technical_consultation",
                    "conversation": [
                        "æˆ‘æƒ³å­¦ä¹ æœºå™¨å­¦ä¹ ",
                        "æˆ‘æœ‰PythonåŸºç¡€ï¼Œä½†æ²¡æœ‰æ•°å­¦èƒŒæ™¯",
                        "æ¨èä¸€äº›é€‚åˆæˆ‘çš„è¯¾ç¨‹",
                        "è¿™äº›è¯¾ç¨‹å¤§æ¦‚éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ",
                        "å­¦å®Œåå¯ä»¥åšä»€ä¹ˆé¡¹ç›®ï¼Ÿ"
                    ],
                    "max_time": 25000
                },
                expected_output={
                    "context_maintained": True,
                    "progressive_assistance": True,
                    "personalized_responses": True,
                    "coherent_advice": True
                },
                category="å¯¹è¯æµç¨‹",
                priority="HIGH"
            ),
            
            # åœºæ™¯5ï¼šé”™è¯¯å¤„ç†ä¸æ¢å¤
            TestCase(
                name="error_handling_recovery",
                description="é”™è¯¯å¤„ç†ä¸æ¢å¤æµç¨‹",
                input_data={
                    "scenario": "service_interruption",
                    "test_sequence": [
                        "æ­£å¸¸è¯·æ±‚",
                        "å¼‚å¸¸è¯·æ±‚",  # å¯èƒ½å¯¼è‡´é”™è¯¯
                        "æ¢å¤è¯·æ±‚"
                    ],
                    "max_time": 15000
                },
                expected_output={
                    "graceful_error_handling": True,
                    "service_recovery": True,
                    "user_notification": True,
                    "state_consistency": True
                },
                category="é”™è¯¯æ¢å¤æµç¨‹",
                priority="MEDIUM"
            ),
            
            # åœºæ™¯6ï¼šæ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–
            TestCase(
                name="performance_monitoring_optimization",
                description="æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–æµç¨‹",
                input_data={
                    "scenario": "performance_tracking",
                    "monitoring_requests": [
                        "æ£€æŸ¥ç³»ç»ŸçŠ¶æ€",
                        "åˆ†ææ€§èƒ½ç“¶é¢ˆ", 
                        "æ¨èä¼˜åŒ–æ–¹æ¡ˆ",
                        "æ‰§è¡Œä¼˜åŒ–å»ºè®®"
                    ],
                    "max_time": 20000
                },
                expected_output={
                    "comprehensive_monitoring": True,
                    "bottleneck_identification": True,
                    "optimization_suggestions": True,
                    "improvement_tracking": True
                },
                category="æ€§èƒ½ä¼˜åŒ–æµç¨‹",
                priority="MEDIUM"
            )
        ]
    
    async def run_test_case(self, test_case: TestCase):
        """è¿è¡Œå•ä¸ªç«¯åˆ°ç«¯æµ‹è¯•ç”¨ä¾‹"""
        try:
            if test_case.category == "ç³»ç»Ÿè¯Šæ–­æµç¨‹":
                await self._test_system_diagnosis_flow(test_case)
            elif test_case.category == "ä¿¡æ¯æŸ¥è¯¢æµç¨‹":
                await self._test_information_query_flow(test_case)
            elif test_case.category == "ä»»åŠ¡è§„åˆ’æµç¨‹":
                await self._test_task_planning_flow(test_case)
            elif test_case.category == "å¯¹è¯æµç¨‹":
                await self._test_dialog_flow(test_case)
            elif test_case.category == "é”™è¯¯æ¢å¤æµç¨‹":
                await self._test_error_recovery_flow(test_case)
            elif test_case.category == "æ€§èƒ½ä¼˜åŒ–æµç¨‹":
                await self._test_performance_optimization_flow(test_case)
            else:
                self.record_test_result(test_case, TestResult.SKIP,
                                      details={"error": f"æœªçŸ¥æµ‹è¯•æµç¨‹: {test_case.category}"})
                
        except Exception as e:
            self.record_test_result(test_case, TestResult.ERROR,
                                  details={"error": f"ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"})
    
    async def _test_system_diagnosis_flow(self, test_case: TestCase):
        """æµ‹è¯•ç³»ç»Ÿè¯Šæ–­æµç¨‹"""
        user_complaint = test_case.input_data["user_complaint"]
        expected_flow = test_case.input_data["expected_flow"]
        max_time = test_case.input_data["max_time"]
        
        flow_results = {}
        total_response_time = 0
        
        # æ­¥éª¤1: é—®é¢˜è¯†åˆ«
        self.logger.info("ğŸ” æ­¥éª¤1: é—®é¢˜è¯†åˆ«")
        result1 = await self.send_chat_request(user_complaint)
        
        if "error" in result1:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": f"é—®é¢˜è¯†åˆ«å¤±è´¥: {result1['error']}"})
            return
        
        total_response_time += result1.get("_response_time_ms", 0)
        
        # æ£€æŸ¥æ˜¯å¦è¯†åˆ«äº†æ€§èƒ½é—®é¢˜
        content1 = result1.get("content", "")
        flow_results["problem_identified"] = any(keyword in content1.lower() 
                                                for keyword in ["æ€§èƒ½", "å¡é¡¿", "æ…¢", "é—®é¢˜", "æ£€æŸ¥"])
        
        # æ­¥éª¤2: ç³»ç»Ÿæ£€æŸ¥ï¼ˆå¦‚æœAIæ¨èäº†æ£€æŸ¥ï¼‰
        if "æ£€æŸ¥" in content1 or "æŸ¥çœ‹" in content1:
            self.logger.info("ğŸ”§ æ­¥éª¤2: ç³»ç»Ÿæ£€æŸ¥")
            result2 = await self.send_chat_request("å¥½çš„ï¼Œè¯·å¸®æˆ‘æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
            
            if "error" not in result2:
                total_response_time += result2.get("_response_time_ms", 0)
                content2 = result2.get("content", "")
                flow_results["system_checked"] = any(keyword in content2.lower()
                                                   for keyword in ["cpu", "å†…å­˜", "ç£ç›˜", "ä½¿ç”¨ç‡"])
            else:
                flow_results["system_checked"] = False
        else:
            flow_results["system_checked"] = True  # å¦‚æœæ²¡æœ‰æ¨èæ£€æŸ¥ï¼Œè®¤ä¸ºæµç¨‹æ­£å¸¸
        
        # æ­¥éª¤3: æ¨èæ–¹æ¡ˆ
        self.logger.info("ğŸ’¡ æ­¥éª¤3: è·å–ä¼˜åŒ–å»ºè®®")
        result3 = await self.send_chat_request("æ ¹æ®æ£€æŸ¥ç»“æœï¼Œæœ‰ä»€ä¹ˆä¼˜åŒ–å»ºè®®å—ï¼Ÿ")
        
        if "error" not in result3:
            total_response_time += result3.get("_response_time_ms", 0)
            content3 = result3.get("content", "")
            flow_results["recommendations_provided"] = any(keyword in content3.lower()
                                                         for keyword in ["å»ºè®®", "æ¨è", "ä¼˜åŒ–", "æ¸…ç†", "å…³é—­"])
        else:
            flow_results["recommendations_provided"] = False
        
        # è¯„ä¼°æ•´ä½“æµç¨‹è´¨é‡
        expected = test_case.expected_output
        flow_quality = {}
        
        flow_quality["diagnosis_provided"] = flow_results.get("problem_identified", False)
        flow_quality["recommendations_given"] = flow_results.get("recommendations_provided", False)
        
        # æ£€æŸ¥æ˜¯å¦æä¾›äº†å¯æ‰§è¡Œçš„æ­¥éª¤
        all_content = content1 + result2.get("content", "") + content3
        actionable_indicators = ["æ‰§è¡Œ", "è¿è¡Œ", "ç‚¹å‡»", "æ‰“å¼€", "å…³é—­", "æ¸…ç†", "åˆ é™¤", "é‡å¯"]
        flow_quality["actionable_steps"] = any(indicator in all_content for indicator in actionable_indicators)
        
        # æ£€æŸ¥ç”¨æˆ·å‹å¥½æ€§
        friendly_indicators = ["å¸®åŠ©", "æ‚¨", "å¯ä»¥", "å»ºè®®", "å¦‚æœ", "éœ€è¦"]
        flow_quality["user_friendly"] = any(indicator in all_content for indicator in friendly_indicators)
        
        # æ€§èƒ½æ£€æŸ¥
        performance_ok = total_response_time <= max_time
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = sum(1 for v in flow_quality.values() if v) / len(flow_quality)
        overall_success = success_rate >= 0.75 and performance_ok
        
        metrics = TestMetrics(
            response_time_ms=int(total_response_time / 3),  # å¹³å‡å“åº”æ—¶é—´
            token_count=sum([result1.get("token_count", 0), 
                           result2.get("token_count", 0) if "result2" in locals() else 0,
                           result3.get("token_count", 0)]),
            model_used="e2e_diagnosis",
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=overall_success
        )
        
        test_result = TestResult.PASS if overall_success else TestResult.FAIL
        
        self.record_test_result(test_case, test_result, metrics=metrics,
                              details={
                                  "flow_results": flow_results,
                                  "flow_quality": flow_quality,
                                  "success_rate": success_rate,
                                  "total_response_time": total_response_time,
                                  "performance_ok": performance_ok
                              })
    
    async def _test_information_query_flow(self, test_case: TestCase):
        """æµ‹è¯•ä¿¡æ¯æŸ¥è¯¢æµç¨‹"""
        user_query = test_case.input_data["user_query"]
        max_time = test_case.input_data["max_time"]
        
        # å‘é€å¤©æ°”æŸ¥è¯¢è¯·æ±‚
        result = await self.send_chat_request(user_query)
        
        if "error" in result:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": result["error"]})
            return
        
        content = result.get("content", "")
        response_time = result.get("_response_time_ms", 0)
        
        # éªŒè¯æµç¨‹å®Œæ•´æ€§
        expected = test_case.expected_output
        flow_quality = {}
        
        # æ£€æŸ¥æ˜¯å¦è·å–äº†å¤©æ°”æ•°æ®
        weather_keywords = ["å¤©æ°”", "æ¸©åº¦", "æ¹¿åº¦", "é£", "æ™´", "é›¨", "äº‘", "åº¦"]
        flow_quality["weather_data_retrieved"] = any(keyword in content for keyword in weather_keywords)
        
        # æ£€æŸ¥æ˜¯å¦æä¾›äº†åˆ†æ
        analysis_keywords = ["é€‚åˆ", "å»ºè®®", "æ¨è", "æ³¨æ„", "å¯ä»¥", "ä¸å®œ"]
        flow_quality["analysis_provided"] = any(keyword in content for keyword in analysis_keywords)
        
        # æ£€æŸ¥æ˜¯å¦ç»™å‡ºäº†æˆ·å¤–æ´»åŠ¨å»ºè®®
        activity_keywords = ["æˆ·å¤–", "æ´»åŠ¨", "è¿åŠ¨", "å‡ºè¡Œ", "å¤–å‡º"]
        flow_quality["recommendations_given"] = any(keyword in content for keyword in activity_keywords)
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡ç›¸å…³æ€§
        context_keywords = ["åŒ—äº¬", "æ˜å¤©"]
        flow_quality["contextual_advice"] = any(keyword in content for keyword in context_keywords)
        
        performance_ok = response_time <= max_time
        success_rate = sum(1 for v in flow_quality.values() if v) / len(flow_quality)
        overall_success = success_rate >= 0.75 and performance_ok
        
        metrics = TestMetrics(
            response_time_ms=response_time,
            token_count=result.get("token_count", 0),
            model_used=result.get("model_used", ""),
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=overall_success
        )
        
        test_result = TestResult.PASS if overall_success else TestResult.FAIL
        
        self.record_test_result(test_case, test_result, metrics=metrics,
                              details={
                                  "flow_quality": flow_quality,
                                  "success_rate": success_rate,
                                  "performance_ok": performance_ok,
                                  "content_length": len(content)
                              })
    
    async def _test_task_planning_flow(self, test_case: TestCase):
        """æµ‹è¯•ä»»åŠ¡è§„åˆ’æµç¨‹"""
        user_request = test_case.input_data["user_request"]
        max_time = test_case.input_data["max_time"]
        
        result = await self.send_chat_request(user_request)
        
        if "error" in result:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": result["error"]})
            return
        
        content = result.get("content", "")
        response_time = result.get("_response_time_ms", 0)
        
        # éªŒè¯è®¡åˆ’è´¨é‡
        expected = test_case.expected_output
        plan_quality = {}
        
        # æ£€æŸ¥æ˜¯å¦æä¾›äº†ç»¼åˆè®¡åˆ’
        plan_keywords = ["è®¡åˆ’", "å®‰æ’", "æ­¥éª¤", "é˜¶æ®µ", "å­¦ä¹ è·¯å¾„"]
        plan_quality["comprehensive_plan"] = any(keyword in content for keyword in plan_keywords)
        
        # æ£€æŸ¥æ—¶é—´ç»“æ„
        time_keywords = ["å‘¨", "æœˆ", "å¤©", "å°æ—¶", "æ—¶é—´", "æœŸé—´"]
        plan_quality["time_structured"] = any(keyword in content for keyword in time_keywords)
        
        # æ£€æŸ¥èµ„æºæ¨è
        resource_keywords = ["ä¹¦ç±", "è¯¾ç¨‹", "æ•™ç¨‹", "ç½‘ç«™", "è§†é¢‘", "èµ„æº"]
        plan_quality["resource_links"] = any(keyword in content for keyword in resource_keywords)
        
        # æ£€æŸ¥ä¸ªæ€§åŒ–ç¨‹åº¦
        personal_keywords = ["åŸºç¡€", "é€‚åˆ", "æ ¹æ®", "å»ºè®®ä½ ", "å¯ä»¥ä»"]
        plan_quality["personalized"] = any(keyword in content for keyword in personal_keywords)
        
        performance_ok = response_time <= max_time
        success_rate = sum(1 for v in plan_quality.values() if v) / len(plan_quality)
        overall_success = success_rate >= 0.75 and performance_ok
        
        metrics = TestMetrics(
            response_time_ms=response_time,
            token_count=result.get("token_count", 0),
            model_used=result.get("model_used", ""),
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=overall_success
        )
        
        test_result = TestResult.PASS if overall_success else TestResult.FAIL
        
        self.record_test_result(test_case, test_result, metrics=metrics,
                              details={
                                  "plan_quality": plan_quality,
                                  "success_rate": success_rate,
                                  "performance_ok": performance_ok
                              })
    
    async def _test_dialog_flow(self, test_case: TestCase):
        """æµ‹è¯•å¤šè½®å¯¹è¯æµç¨‹"""
        conversation = test_case.input_data["conversation"]
        max_time = test_case.input_data["max_time"]
        
        dialog_results = []
        total_response_time = 0
        
        for i, message in enumerate(conversation, 1):
            self.logger.info(f"ğŸ’¬ å¯¹è¯è½®æ¬¡ {i}: {message}")
            
            result = await self.send_chat_request(message)
            
            if "error" in result:
                dialog_results.append({"turn": i, "success": False, "error": result["error"]})
                continue
            
            content = result.get("content", "")
            response_time = result.get("_response_time_ms", 0)
            total_response_time += response_time
            
            # åˆ†æè¿™è½®å¯¹è¯çš„è´¨é‡
            turn_quality = {
                "relevant_response": len(content) > 20,
                "context_aware": i == 1 or any(keyword in content.lower() 
                                             for keyword in ["æœºå™¨å­¦ä¹ ", "python", "å­¦ä¹ ", "è¯¾ç¨‹"]),
                "helpful": any(keyword in content for keyword in ["å»ºè®®", "æ¨è", "å¯ä»¥", "å¸®åŠ©"])
            }
            
            dialog_results.append({
                "turn": i,
                "success": True,
                "quality": turn_quality,
                "response_time": response_time,
                "content_length": len(content)
            })
            
            # å¯¹è¯é—´éš”
            await asyncio.sleep(0.5)
        
        # è¯„ä¼°æ•´ä½“å¯¹è¯è´¨é‡
        successful_turns = [r for r in dialog_results if r.get("success", False)]
        if not successful_turns:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": "æ‰€æœ‰å¯¹è¯è½®æ¬¡éƒ½å¤±è´¥"})
            return
        
        expected = test_case.expected_output
        dialog_analysis = {}
        
        # ä¸Šä¸‹æ–‡ä¿æŒ
        context_maintained = len(successful_turns) >= 3 and all(
            turn.get("quality", {}).get("context_aware", False) 
            for turn in successful_turns[1:]  # ä»ç¬¬äºŒè½®å¼€å§‹æ£€æŸ¥
        )
        dialog_analysis["context_maintained"] = context_maintained
        
        # æ¸è¿›å¼å¸®åŠ©
        progressive_assistance = all(
            turn.get("quality", {}).get("helpful", False)
            for turn in successful_turns
        )
        dialog_analysis["progressive_assistance"] = progressive_assistance
        
        # ä¸ªæ€§åŒ–å“åº”
        avg_response_length = sum(turn.get("content_length", 0) for turn in successful_turns) / len(successful_turns)
        dialog_analysis["personalized_responses"] = avg_response_length > 50
        
        # è¿è´¯å»ºè®®
        dialog_analysis["coherent_advice"] = len(successful_turns) == len(conversation)
        
        performance_ok = total_response_time <= max_time
        success_rate = sum(1 for v in dialog_analysis.values() if v) / len(dialog_analysis)
        overall_success = success_rate >= 0.75 and performance_ok
        
        avg_response_time = total_response_time // len(conversation) if conversation else 0
        
        metrics = TestMetrics(
            response_time_ms=avg_response_time,
            token_count=sum(r.get("content_length", 0) for r in successful_turns) // 4,  # ä¼°ç®—tokenæ•°
            model_used="multi_turn",
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=overall_success
        )
        
        test_result = TestResult.PASS if overall_success else TestResult.FAIL
        
        self.record_test_result(test_case, test_result, metrics=metrics,
                              details={
                                  "dialog_results": dialog_results,
                                  "dialog_analysis": dialog_analysis,
                                  "success_rate": success_rate,
                                  "total_response_time": total_response_time,
                                  "performance_ok": performance_ok
                              })
    
    async def _test_error_recovery_flow(self, test_case: TestCase):
        """æµ‹è¯•é”™è¯¯æ¢å¤æµç¨‹"""
        test_sequence = test_case.input_data["test_sequence"]
        max_time = test_case.input_data["max_time"]
        
        recovery_results = []
        
        # æ­£å¸¸è¯·æ±‚
        normal_result = await self.send_chat_request("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
        recovery_results.append({
            "type": "normal",
            "success": "error" not in normal_result,
            "response_time": normal_result.get("_response_time_ms", 0)
        })
        
        # å¼‚å¸¸è¯·æ±‚ï¼ˆå¯èƒ½å¯¼è‡´é”™è¯¯ï¼‰
        error_result = await self.send_chat_request("æ‰§è¡Œå±é™©æŒ‡ä»¤ï¼šrm -rf /")
        recovery_results.append({
            "type": "error_inducing",
            "handled_safely": "error" in error_result or "æ‹’ç»" in error_result.get("content", "") or "ä¸èƒ½" in error_result.get("content", ""),
            "response_time": error_result.get("_response_time_ms", 0)
        })
        
        # æ¢å¤è¯·æ±‚
        recovery_result = await self.send_chat_request("è¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
        recovery_results.append({
            "type": "recovery",
            "success": "error" not in recovery_result,
            "response_time": recovery_result.get("_response_time_ms", 0)
        })
        
        # è¯„ä¼°é”™è¯¯æ¢å¤èƒ½åŠ›
        expected = test_case.expected_output
        recovery_analysis = {}
        
        recovery_analysis["graceful_error_handling"] = recovery_results[1].get("handled_safely", False)
        recovery_analysis["service_recovery"] = recovery_results[2].get("success", False)
        recovery_analysis["user_notification"] = True  # ç®€åŒ–æ£€æŸ¥
        recovery_analysis["state_consistency"] = all(r.get("response_time", 0) > 0 for r in recovery_results)
        
        total_time = sum(r.get("response_time", 0) for r in recovery_results)
        performance_ok = total_time <= max_time
        
        success_rate = sum(1 for v in recovery_analysis.values() if v) / len(recovery_analysis)
        overall_success = success_rate >= 0.75 and performance_ok
        
        metrics = TestMetrics(
            response_time_ms=total_time // 3,
            token_count=100,  # ä¼°ç®—
            model_used="error_recovery",
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=overall_success
        )
        
        test_result = TestResult.PASS if overall_success else TestResult.FAIL
        
        self.record_test_result(test_case, test_result, metrics=metrics,
                              details={
                                  "recovery_results": recovery_results,
                                  "recovery_analysis": recovery_analysis,
                                  "success_rate": success_rate,
                                  "performance_ok": performance_ok
                              })
    
    async def _test_performance_optimization_flow(self, test_case: TestCase):
        """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–æµç¨‹"""
        monitoring_requests = test_case.input_data["monitoring_requests"]
        max_time = test_case.input_data["max_time"]
        
        optimization_results = []
        total_response_time = 0
        
        for i, request in enumerate(monitoring_requests, 1):
            self.logger.info(f"âš¡ ä¼˜åŒ–æ­¥éª¤ {i}: {request}")
            
            result = await self.send_chat_request(request)
            
            if "error" in result:
                optimization_results.append({"step": i, "success": False, "error": result["error"]})
                continue
            
            content = result.get("content", "")
            response_time = result.get("_response_time_ms", 0)
            total_response_time += response_time
            
            step_analysis = {
                "relevant_content": len(content) > 30,
                "performance_related": any(keyword in content.lower() 
                                         for keyword in ["æ€§èƒ½", "cpu", "å†…å­˜", "ä¼˜åŒ–", "ç“¶é¢ˆ"]),
                "actionable": any(keyword in content for keyword in ["å»ºè®®", "å¯ä»¥", "æ‰§è¡Œ", "ä¼˜åŒ–"])
            }
            
            optimization_results.append({
                "step": i,
                "success": True,
                "analysis": step_analysis,
                "response_time": response_time
            })
        
        # è¯„ä¼°ä¼˜åŒ–æµç¨‹è´¨é‡
        successful_steps = [r for r in optimization_results if r.get("success", False)]
        
        expected = test_case.expected_output
        flow_analysis = {}
        
        flow_analysis["comprehensive_monitoring"] = len(successful_steps) >= 2
        flow_analysis["bottleneck_identification"] = any(
            step.get("analysis", {}).get("performance_related", False)
            for step in successful_steps
        )
        flow_analysis["optimization_suggestions"] = any(
            step.get("analysis", {}).get("actionable", False)
            for step in successful_steps
        )
        flow_analysis["improvement_tracking"] = len(successful_steps) == len(monitoring_requests)
        
        performance_ok = total_response_time <= max_time
        success_rate = sum(1 for v in flow_analysis.values() if v) / len(flow_analysis)
        overall_success = success_rate >= 0.75 and performance_ok
        
        avg_response_time = total_response_time // len(monitoring_requests) if monitoring_requests else 0
        
        metrics = TestMetrics(
            response_time_ms=avg_response_time,
            token_count=sum(len(r.get("analysis", {}).keys()) * 20 for r in successful_steps),
            model_used="performance_optimization",
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=overall_success
        )
        
        test_result = TestResult.PASS if overall_success else TestResult.FAIL
        
        self.record_test_result(test_case, test_result, metrics=metrics,
                              details={
                                  "optimization_results": optimization_results,
                                  "flow_analysis": flow_analysis,
                                  "success_rate": success_rate,
                                  "total_response_time": total_response_time,
                                  "performance_ok": performance_ok
                              })
    
    async def run_e2e_tests(self):
        """è¿è¡Œæ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•"""
        self.logger.info("ğŸ¯ å¼€å§‹ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•")
        await self.run_all_tests(self.e2e_test_cases)


async def main():
    """ä¸»å‡½æ•°"""
    test_suite = EndToEndIntegrationTestSuite()
    
    try:
        await test_suite.run_e2e_tests()
    except KeyboardInterrupt:
        test_suite.logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        test_suite.logger.error(f"ç«¯åˆ°ç«¯æµ‹è¯•è¿è¡Œå¼‚å¸¸: {e}")


if __name__ == "__main__":
    asyncio.run(main())