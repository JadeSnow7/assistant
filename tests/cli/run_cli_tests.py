#!/usr/bin/env python3
"""
CLIæµ‹è¯•å¥—ä»¶è¿è¡Œå™¨
ç»Ÿä¸€è¿è¡Œæ‰€æœ‰CLIç›¸å…³æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š
"""
import asyncio
import argparse
import json
import time
from pathlib import Path
from typing import Dict, Any, List
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from tests.cli.test_cli_commands import CLICommandRoutingTestSuite, CLIDisplayTestSuite
from tests.cli.test_cli_performance import CLIPerformanceTestSuite, CLICompatibilityTestSuite


class CLITestRunner:
    """CLIæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000", output_dir: str = "test_reports"):
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # æ³¨å†Œæµ‹è¯•å¥—ä»¶
        self.test_suites = {
            "commands": CLICommandRoutingTestSuite(base_url),
            "display": CLIDisplayTestSuite(base_url),
            "performance": CLIPerformanceTestSuite(base_url),
            "compatibility": CLICompatibilityTestSuite(base_url)
        }
        
        self.overall_results = {}
        self.start_time = None
        self.end_time = None
    
    async def run_health_check(self) -> bool:
        """è¿è¡Œå¥åº·æ£€æŸ¥"""
        print("ğŸ¥ æ‰§è¡ŒCLIæµ‹è¯•ç¯å¢ƒå¥åº·æ£€æŸ¥...")
        
        try:
            # æ£€æŸ¥Pythonç¯å¢ƒ
            python_version = sys.version_info
            if python_version < (3, 9):
                print(f"âŒ Pythonç‰ˆæœ¬è¿‡ä½: {python_version.major}.{python_version.minor} (éœ€è¦ >= 3.9)")
                return False
            
            # æ£€æŸ¥å…³é”®æ¨¡å—
            required_modules = ["rich", "textual", "asyncio", "pathlib"]
            for module_name in required_modules:
                try:
                    __import__(module_name)
                except ImportError as e:
                    print(f"âŒ ç¼ºå°‘å¿…éœ€æ¨¡å—: {module_name} - {e}")
                    return False
            
            # æ£€æŸ¥CLIè„šæœ¬æ–‡ä»¶
            cli_scripts = [
                project_root / "start_cli.py",
                project_root / "ui" / "cli" / "modern_cli.py"
            ]
            
            for script in cli_scripts:
                if not script.exists():
                    print(f"âŒ CLIè„šæœ¬ä¸å­˜åœ¨: {script}")
                    return False
            
            print("âœ… CLIæµ‹è¯•ç¯å¢ƒå¥åº·æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    async def run_single_suite(self, suite_name: str, suite) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¥—ä»¶"""
        print(f"\nğŸ§ª å¼€å§‹è¿è¡ŒCLIæµ‹è¯•å¥—ä»¶: {suite_name}")
        print("=" * 60)
        
        suite_start_time = time.time()
        
        try:
            # æ ¹æ®å¥—ä»¶ç±»å‹è¿è¡Œç›¸åº”çš„æµ‹è¯•æ–¹æ³•
            if suite_name == "commands":
                await suite.run_command_routing_tests()
            elif suite_name == "display":
                await suite.run_display_tests()
            elif suite_name == "performance":
                await suite.run_performance_tests()
            elif suite_name == "compatibility":
                await suite.run_compatibility_tests()
            
            suite_end_time = time.time()
            suite_duration = suite_end_time - suite_start_time
            
            # è·å–æµ‹è¯•æŠ¥å‘Š
            report = suite.generate_test_report()
            report["suite_name"] = suite_name
            report["duration_seconds"] = round(suite_duration, 2)
            report["timestamp"] = time.time()
            
            print(f"âœ… CLIæµ‹è¯•å¥—ä»¶ {suite_name} å®Œæˆ")
            print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {suite_duration:.2f}ç§’")
            
            return report
            
        except Exception as e:
            suite_end_time = time.time()
            suite_duration = suite_end_time - suite_start_time
            
            error_report = {
                "suite_name": suite_name,
                "error": str(e),
                "duration_seconds": round(suite_duration, 2),
                "timestamp": time.time(),
                "summary": {
                    "total_tests": 0,
                    "passed": 0,
                    "failed": 0,
                    "errors": 1,
                    "skipped": 0,
                    "success_rate": "0%"
                }
            }
            
            print(f"âŒ CLIæµ‹è¯•å¥—ä»¶ {suite_name} æ‰§è¡Œå¤±è´¥: {e}")
            return error_report
    
    async def run_all_suites(self, selected_suites: List[str] = None) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶"""
        print("ğŸš€ å¼€å§‹AI Assistant CLIå®Œæ•´æµ‹è¯•")
        print("=" * 80)
        
        self.start_time = time.time()
        
        # ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•å¥—ä»¶
        if selected_suites:
            suites_to_run = {name: suite for name, suite in self.test_suites.items() 
                           if name in selected_suites}
        else:
            suites_to_run = self.test_suites
        
        # è¿è¡Œæ¯ä¸ªæµ‹è¯•å¥—ä»¶
        for suite_name, suite in suites_to_run.items():
            suite_report = await self.run_single_suite(suite_name, suite)
            self.overall_results[suite_name] = suite_report
        
        self.end_time = time.time()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        overall_report = self._generate_overall_report()
        
        return overall_report
    
    def _generate_overall_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        total_duration = self.end_time - self.start_time if self.start_time and self.end_time else 0
        
        # æ±‡æ€»ç»Ÿè®¡
        overall_stats = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "errors": 0,
            "skipped": 0
        }
        
        suite_summaries = {}
        performance_metrics = {
            "total_response_times": [],
            "suite_durations": [],
            "average_response_time": 0,
            "total_execution_time": total_duration
        }
        
        cli_specific_metrics = {
            "startup_times": [],
            "response_times": [],
            "memory_usage": [],
            "compatibility_scores": []
        }
        
        # æ±‡æ€»å„å¥—ä»¶ç»“æœ
        for suite_name, report in self.overall_results.items():
            if "summary" in report:
                summary = report["summary"]
                overall_stats["total_tests"] += summary.get("total_tests", 0)
                overall_stats["passed"] += summary.get("passed", 0)
                overall_stats["failed"] += summary.get("failed", 0)
                overall_stats["errors"] += summary.get("errors", 0)
                overall_stats["skipped"] += summary.get("skipped", 0)
                
                suite_summaries[suite_name] = {
                    "tests": summary.get("total_tests", 0),
                    "success_rate": summary.get("success_rate", "0%"),
                    "duration": report.get("duration_seconds", 0)
                }
                
                # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
                performance_metrics["suite_durations"].append(report.get("duration_seconds", 0))
                
                # CLIç‰¹å®šæŒ‡æ ‡
                if "performance" in report and "test_details" in report:
                    for test in report["test_details"]:
                        test_name = test.get("test_name", "")
                        details = test.get("details", {})
                        
                        if "startup" in test_name:
                            if "average_startup_time" in details:
                                cli_specific_metrics["startup_times"].append(details["average_startup_time"])
                        elif "response" in test_name:
                            if "average_response_time" in details:
                                cli_specific_metrics["response_times"].append(details["average_response_time"])
                        elif "memory" in test_name:
                            if "memory_mb" in details:
                                cli_specific_metrics["memory_usage"].append(details["memory_mb"])
        
        # è®¡ç®—å¹³å‡å€¼
        if performance_metrics["suite_durations"]:
            performance_metrics["average_suite_duration"] = sum(performance_metrics["suite_durations"]) / len(performance_metrics["suite_durations"])
        
        if cli_specific_metrics["startup_times"]:
            cli_specific_metrics["average_startup_time"] = sum(cli_specific_metrics["startup_times"]) / len(cli_specific_metrics["startup_times"])
        
        if cli_specific_metrics["response_times"]:
            cli_specific_metrics["average_response_time"] = sum(cli_specific_metrics["response_times"]) / len(cli_specific_metrics["response_times"])
        
        if cli_specific_metrics["memory_usage"]:
            cli_specific_metrics["average_memory_usage"] = sum(cli_specific_metrics["memory_usage"]) / len(cli_specific_metrics["memory_usage"])
        
        # è®¡ç®—æˆåŠŸç‡
        if overall_stats["total_tests"] > 0:
            success_rate = (overall_stats["passed"] / overall_stats["total_tests"]) * 100
            overall_stats["success_rate"] = f"{success_rate:.1f}%"
        else:
            overall_stats["success_rate"] = "0%"
        
        return {
            "test_type": "CLI Complete Test Suite",
            "timestamp": self.end_time,
            "total_duration": total_duration,
            "summary": overall_stats,
            "suite_summaries": suite_summaries,
            "performance_metrics": performance_metrics,
            "cli_specific_metrics": cli_specific_metrics,
            "detailed_results": self.overall_results,
            "environment": {
                "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
                "platform": sys.platform,
                "base_url": self.base_url
            }
        }
    
    def save_reports(self, overall_report: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        timestamp = int(time.time())
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        overall_report_file = self.output_dir / f"cli_test_report_{timestamp}.json"
        with open(overall_report_file, "w", encoding="utf-8") as f:
            json.dump(overall_report, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å„å¥—ä»¶è¯¦ç»†æŠ¥å‘Š
        for suite_name, report in self.overall_results.items():
            suite_report_file = self.output_dir / f"cli_{suite_name}_report_{timestamp}.json"
            with open(suite_report_file, "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self._generate_html_report(overall_report, timestamp)
        
        print(f"\nğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.output_dir}")
        print(f"   - ç»¼åˆæŠ¥å‘Š: cli_test_report_{timestamp}.json")
        print(f"   - HTMLæŠ¥å‘Š: cli_test_report_{timestamp}.html")
    
    def _generate_html_report(self, overall_report: Dict[str, Any], timestamp: int):
        """ç”ŸæˆHTMLæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Assistant CLIæµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1, h2, h3 {{ color: #333; }}
        .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric {{ background: #f8f9fa; padding: 15px; border-radius: 5px; text-align: center; }}
        .metric h3 {{ margin: 0 0 10px 0; color: #495057; }}
        .metric .value {{ font-size: 24px; font-weight: bold; color: #007bff; }}
        .success {{ color: #28a745; }}
        .warning {{ color: #ffc107; }}
        .danger {{ color: #dc3545; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #f8f9fa; }}
        .suite-details {{ margin: 20px 0; }}
        .suite-name {{ background: #e9ecef; padding: 10px; border-radius: 5px; margin: 10px 0; }}
        pre {{ background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ§ª AI Assistant CLIæµ‹è¯•æŠ¥å‘Š</h1>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}</p>
        <p><strong>æ‰§è¡Œæ—¶é•¿:</strong> {overall_report['total_duration']:.2f}ç§’</p>
        
        <div class="summary">
            <div class="metric">
                <h3>æ€»æµ‹è¯•æ•°</h3>
                <div class="value">{overall_report['summary']['total_tests']}</div>
            </div>
            <div class="metric">
                <h3>é€šè¿‡</h3>
                <div class="value success">{overall_report['summary']['passed']}</div>
            </div>
            <div class="metric">
                <h3>å¤±è´¥</h3>
                <div class="value danger">{overall_report['summary']['failed']}</div>
            </div>
            <div class="metric">
                <h3>æˆåŠŸç‡</h3>
                <div class="value">{overall_report['summary']['success_rate']}</div>
            </div>
        </div>
        
        <h2>ğŸ“‹ æµ‹è¯•å¥—ä»¶æ‘˜è¦</h2>
        <table>
            <thead>
                <tr>
                    <th>æµ‹è¯•å¥—ä»¶</th>
                    <th>æµ‹è¯•æ•°é‡</th>
                    <th>æˆåŠŸç‡</th>
                    <th>æ‰§è¡Œæ—¶é—´(ç§’)</th>
                </tr>
            </thead>
            <tbody>
        """
        
        for suite_name, summary in overall_report["suite_summaries"].items():
            html_content += f"""
                <tr>
                    <td>{suite_name}</td>
                    <td>{summary['tests']}</td>
                    <td>{summary['success_rate']}</td>
                    <td>{summary['duration']:.2f}</td>
                </tr>
            """
        
        html_content += """
            </tbody>
        </table>
        
        <h2>âš¡ CLIæ€§èƒ½æŒ‡æ ‡</h2>
        <div class="summary">
        """
        
        cli_metrics = overall_report.get("cli_specific_metrics", {})
        if cli_metrics.get("average_startup_time"):
            html_content += f"""
            <div class="metric">
                <h3>å¹³å‡å¯åŠ¨æ—¶é—´</h3>
                <div class="value">{cli_metrics['average_startup_time']:.2f}s</div>
            </div>
            """
        
        if cli_metrics.get("average_response_time"):
            html_content += f"""
            <div class="metric">
                <h3>å¹³å‡å“åº”æ—¶é—´</h3>
                <div class="value">{cli_metrics['average_response_time']:.2f}s</div>
            </div>
            """
        
        if cli_metrics.get("average_memory_usage"):
            html_content += f"""
            <div class="metric">
                <h3>å¹³å‡å†…å­˜ä½¿ç”¨</h3>
                <div class="value">{cli_metrics['average_memory_usage']:.1f}MB</div>
            </div>
            """
        
        html_content += """
        </div>
        
        <h2>ğŸ” è¯¦ç»†ç»“æœ</h2>
        """
        
        for suite_name, report in overall_report["detailed_results"].items():
            html_content += f"""
            <div class="suite-details">
                <div class="suite-name">
                    <h3>{suite_name.upper()} æµ‹è¯•å¥—ä»¶</h3>
                </div>
            """
            
            if "test_details" in report:
                html_content += """
                <table>
                    <thead>
                        <tr>
                            <th>æµ‹è¯•åç§°</th>
                            <th>çŠ¶æ€</th>
                            <th>å“åº”æ—¶é—´</th>
                            <th>æ¶ˆæ¯</th>
                        </tr>
                    </thead>
                    <tbody>
                """
                
                for test in report["test_details"]:
                    status_class = "success" if test.get("success") else "danger"
                    status_text = "é€šè¿‡" if test.get("success") else "å¤±è´¥"
                    
                    html_content += f"""
                        <tr>
                            <td>{test.get('test_name', 'Unknown')}</td>
                            <td class="{status_class}">{status_text}</td>
                            <td>{test.get('response_time', 0):.3f}s</td>
                            <td>{test.get('message', '')}</td>
                        </tr>
                    """
                
                html_content += """
                    </tbody>
                </table>
                """
            
            html_content += "</div>"
        
        html_content += """
        </div>
    </div>
</body>
</html>
        """
        
        html_report_file = self.output_dir / f"cli_test_report_{timestamp}.html"
        with open(html_report_file, "w", encoding="utf-8") as f:
            f.write(html_content)
    
    def print_summary(self, overall_report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ§ª AI Assistant CLIæµ‹è¯•å®Œæ•´æŠ¥å‘Š")
        print("="*80)
        
        summary = overall_report["summary"]
        print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"âœ… é€šè¿‡: {summary['passed']}")
        print(f"âŒ å¤±è´¥: {summary['failed']}")
        print(f"âš ï¸ é”™è¯¯: {summary['errors']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']}")
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {overall_report['total_duration']:.2f}ç§’")
        
        print("\nğŸ“‹ å„å¥—ä»¶ç»“æœ:")
        for suite_name, summary in overall_report["suite_summaries"].items():
            print(f"  {suite_name}: {summary['tests']}æµ‹è¯•, {summary['success_rate']}é€šè¿‡ç‡, {summary['duration']:.2f}s")
        
        # CLIç‰¹å®šæŒ‡æ ‡
        cli_metrics = overall_report.get("cli_specific_metrics", {})
        if cli_metrics:
            print("\nâš¡ CLIæ€§èƒ½æŒ‡æ ‡:")
            if cli_metrics.get("average_startup_time"):
                print(f"  å¹³å‡å¯åŠ¨æ—¶é—´: {cli_metrics['average_startup_time']:.2f}s")
            if cli_metrics.get("average_response_time"):
                print(f"  å¹³å‡å“åº”æ—¶é—´: {cli_metrics['average_response_time']:.2f}s")
            if cli_metrics.get("average_memory_usage"):
                print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {cli_metrics['average_memory_usage']:.1f}MB")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AI Assistant CLIæµ‹è¯•å¥—ä»¶")
    parser.add_argument("--url", default="http://localhost:8000", help="APIæœåŠ¡åœ°å€")
    parser.add_argument("--output", default="test_reports", help="æµ‹è¯•æŠ¥å‘Šè¾“å‡ºç›®å½•")
    parser.add_argument("--suites", nargs="+", 
                       choices=["commands", "display", "performance", "compatibility"],
                       help="æŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•å¥—ä»¶")
    parser.add_argument("--no-health-check", action="store_true", help="è·³è¿‡å¥åº·æ£€æŸ¥")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = CLITestRunner(args.url, args.output)
    
    # å¥åº·æ£€æŸ¥
    if not args.no_health_check:
        if not await runner.run_health_check():
            print("âŒ å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
            sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    try:
        overall_report = await runner.run_all_suites(args.suites)
        
        # æ˜¾ç¤ºæ‘˜è¦
        runner.print_summary(overall_report)
        
        # ä¿å­˜æŠ¥å‘Š
        runner.save_reports(overall_report)
        
        # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
        if overall_report["summary"]["failed"] > 0 or overall_report["summary"]["errors"] > 0:
            sys.exit(1)
        else:
            sys.exit(0)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())