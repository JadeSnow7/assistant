#!/usr/bin/env python3
"""
CLIæ€§èƒ½å’Œå…¼å®¹æ€§æµ‹è¯•
æµ‹è¯•CLIçš„å¯åŠ¨æ€§èƒ½ã€å“åº”æ—¶é—´å’Œè·¨å¹³å°å…¼å®¹æ€§
"""
import asyncio
import sys
import time
import platform
import subprocess
from pathlib import Path
from typing import Dict, Any, List
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from tests.unit.cli_test_base import CLITestBase, CLIPerformanceTest, CLICompatibilityTest, TestResult


class CLIPerformanceTestSuite(CLIPerformanceTest):
    """CLIæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        super().__init__(base_url)
        self.performance_benchmarks = {
            "startup_time_threshold": 3.0,  # ç§’
            "response_time_threshold": 2.0,  # ç§’
            "memory_usage_threshold": 100,   # MB
            "cpu_usage_threshold": 50        # %
        }
    
    async def run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print("âš¡ å¼€å§‹CLIæ€§èƒ½æµ‹è¯•")
        
        await self.setup_test_environment()
        
        try:
            # å¯åŠ¨æ—¶é—´æµ‹è¯•
            startup_results = await self._test_startup_performance()
            self.test_results.extend(startup_results)
            
            # å“åº”æ—¶é—´æµ‹è¯•
            response_results = await self._test_response_performance()
            self.test_results.extend(response_results)
            
            # å†…å­˜ä½¿ç”¨æµ‹è¯•
            memory_results = await self._test_memory_usage()
            self.test_results.extend(memory_results)
            
            # å¹¶å‘æ€§èƒ½æµ‹è¯•
            concurrent_results = await self._test_concurrent_performance()
            self.test_results.extend(concurrent_results)
            
        finally:
            await self.teardown_test_environment()
    
    async def _test_startup_performance(self) -> List[TestResult]:
        """æµ‹è¯•å¯åŠ¨æ€§èƒ½"""
        results = []
        
        cli_scripts = [
            "start_cli.py",
            "ui/cli/modern_cli.py"
        ]
        
        for script in cli_scripts:
            test_name = f"startup_performance_{script.replace('/', '_').replace('.py', '')}"
            
            try:
                # å¤šæ¬¡æµ‹é‡å–å¹³å‡å€¼
                startup_times = []
                for i in range(5):
                    result = await self.measure_startup_time(script)
                    if result["success"]:
                        startup_times.append(result["startup_time"])
                
                if startup_times:
                    avg_startup_time = sum(startup_times) / len(startup_times)
                    min_startup_time = min(startup_times)
                    max_startup_time = max(startup_times)
                    
                    success = avg_startup_time <= self.performance_benchmarks["startup_time_threshold"]
                    
                    results.append(TestResult(
                        test_name=test_name,
                        success=success,
                        response_time=avg_startup_time,
                        message=f"å¹³å‡å¯åŠ¨æ—¶é—´: {avg_startup_time:.2f}s",
                        details={
                            "script": script,
                            "average_startup_time": avg_startup_time,
                            "min_startup_time": min_startup_time,
                            "max_startup_time": max_startup_time,
                            "threshold": self.performance_benchmarks["startup_time_threshold"],
                            "all_times": startup_times
                        }
                    ))
                else:
                    results.append(TestResult(
                        test_name=test_name,
                        success=False,
                        response_time=0,
                        message=f"æ— æ³•æµ‹é‡ {script} çš„å¯åŠ¨æ—¶é—´",
                        details={"script": script, "error": "no_successful_measurements"}
                    ))
            
            except Exception as e:
                results.append(TestResult(
                    test_name=test_name,
                    success=False,
                    response_time=0,
                    message=f"å¯åŠ¨æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}",
                    details={"script": script, "error": str(e)}
                ))
        
        return results
    
    async def _test_response_performance(self) -> List[TestResult]:
        """æµ‹è¯•å“åº”æ€§èƒ½"""
        results = []
        
        test_inputs = [
            "ä½ å¥½",
            "help",
            "è¿™æ˜¯ä¸€ä¸ªè¾ƒé•¿çš„æµ‹è¯•æ¶ˆæ¯ï¼Œç”¨æ¥æµ‹è¯•ç³»ç»Ÿå¤„ç†é•¿æ–‡æœ¬çš„å“åº”æ—¶é—´",
            "/status",
            "/health"
        ]
        
        scripts = ["start_cli.py", "ui/cli/modern_cli.py"]
        
        for script in scripts:
            test_name = f"response_performance_{script.replace('/', '_').replace('.py', '')}"
            
            try:
                result = await self.measure_response_time(script, test_inputs)
                avg_response_time = result["average_response_time"]
                
                success = avg_response_time <= self.performance_benchmarks["response_time_threshold"]
                
                results.append(TestResult(
                    test_name=test_name,
                    success=success,
                    response_time=avg_response_time,
                    message=f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s",
                    details={
                        "script": script,
                        "average_response_time": avg_response_time,
                        "response_times": result["response_times"],
                        "threshold": self.performance_benchmarks["response_time_threshold"],
                        "total_tests": result["total_tests"]
                    }
                ))
            
            except Exception as e:
                results.append(TestResult(
                    test_name=test_name,
                    success=False,
                    response_time=0,
                    message=f"å“åº”æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}",
                    details={"script": script, "error": str(e)}
                ))
        
        return results
    
    async def _test_memory_usage(self) -> List[TestResult]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        results = []
        
        try:
            import psutil
            
            # æµ‹è¯•ä¸åŒCLIè„šæœ¬çš„å†…å­˜ä½¿ç”¨
            scripts = ["start_cli.py", "ui/cli/modern_cli.py"]
            
            for script in scripts:
                test_name = f"memory_usage_{script.replace('/', '_').replace('.py', '')}"
                
                try:
                    # å¯åŠ¨CLIè¿›ç¨‹
                    process = subprocess.Popen(
                        [sys.executable, script],
                        cwd=str(project_root),
                        stdin=subprocess.PIPE,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True
                    )
                    
                    # ç­‰å¾…è¿›ç¨‹ç¨³å®š
                    await asyncio.sleep(2)
                    
                    # æµ‹é‡å†…å­˜ä½¿ç”¨
                    if process.poll() is None:  # è¿›ç¨‹ä»åœ¨è¿è¡Œ
                        ps_process = psutil.Process(process.pid)
                        memory_info = ps_process.memory_info()
                        memory_mb = memory_info.rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
                        
                        success = memory_mb <= self.performance_benchmarks["memory_usage_threshold"]
                        
                        results.append(TestResult(
                            test_name=test_name,
                            success=success,
                            response_time=0,
                            message=f"å†…å­˜ä½¿ç”¨: {memory_mb:.1f}MB",
                            details={
                                "script": script,
                                "memory_mb": memory_mb,
                                "memory_bytes": memory_info.rss,
                                "threshold_mb": self.performance_benchmarks["memory_usage_threshold"]
                            }
                        ))
                    else:
                        results.append(TestResult(
                            test_name=test_name,
                            success=False,
                            response_time=0,
                            message=f"è¿›ç¨‹ {script} æå‰é€€å‡º",
                            details={"script": script, "error": "process_exited_early"}
                        ))
                    
                    # æ¸…ç†è¿›ç¨‹
                    if process.poll() is None:
                        process.terminate()
                        try:
                            process.wait(timeout=5)
                        except subprocess.TimeoutExpired:
                            process.kill()
                
                except Exception as e:
                    results.append(TestResult(
                        test_name=test_name,
                        success=False,
                        response_time=0,
                        message=f"å†…å­˜ä½¿ç”¨æµ‹è¯•å¤±è´¥: {str(e)}",
                        details={"script": script, "error": str(e)}
                    ))
        
        except ImportError:
            results.append(TestResult(
                test_name="memory_usage_psutil_missing",
                success=False,
                response_time=0,
                message="psutilæ¨¡å—æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œå†…å­˜ä½¿ç”¨æµ‹è¯•",
                details={"error": "psutil_not_installed"}
            ))
        
        return results
    
    async def _test_concurrent_performance(self) -> List[TestResult]:
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        results = []
        
        # æ¨¡æ‹Ÿå¤šä¸ªå¹¶å‘CLIä¼šè¯
        concurrent_sessions = [2, 4, 8]
        
        for session_count in concurrent_sessions:
            test_name = f"concurrent_performance_{session_count}_sessions"
            start_time = time.time()
            
            try:
                # åˆ›å»ºå¤šä¸ªå¹¶å‘ä»»åŠ¡
                tasks = []
                for i in range(session_count):
                    task = self.run_cli_command(
                        "python start_cli.py",
                        [f"æµ‹è¯•ä¼šè¯{i}", "exit"],
                        timeout=15
                    )
                    tasks.append(task)
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                results_list = await asyncio.gather(*tasks, return_exceptions=True)
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # åˆ†æç»“æœ
                successful_sessions = sum(1 for r in results_list if isinstance(r, dict) and r.get("success", False))
                success_rate = successful_sessions / session_count
                
                success = success_rate >= 0.8  # 80%ä»¥ä¸ŠæˆåŠŸç‡è®¤ä¸ºé€šè¿‡
                
                results.append(TestResult(
                    test_name=test_name,
                    success=success,
                    response_time=total_time,
                    message=f"å¹¶å‘{session_count}ä¼šè¯: {successful_sessions}/{session_count}æˆåŠŸ",
                    details={
                        "session_count": session_count,
                        "successful_sessions": successful_sessions,
                        "success_rate": success_rate,
                        "total_time": total_time,
                        "avg_time_per_session": total_time / session_count,
                        "results": [r if not isinstance(r, Exception) else str(r) for r in results_list]
                    }
                ))
            
            except Exception as e:
                results.append(TestResult(
                    test_name=test_name,
                    success=False,
                    response_time=time.time() - start_time,
                    message=f"å¹¶å‘æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}",
                    details={"session_count": session_count, "error": str(e)}
                ))
        
        return results


class CLICompatibilityTestSuite(CLICompatibilityTest):
    """CLIå…¼å®¹æ€§æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        super().__init__(base_url)
    
    async def run_compatibility_tests(self):
        """è¿è¡Œå…¼å®¹æ€§æµ‹è¯•"""
        print("ğŸ”„ å¼€å§‹CLIå…¼å®¹æ€§æµ‹è¯•")
        
        await self.setup_test_environment()
        
        try:
            # Pythonç‰ˆæœ¬å…¼å®¹æ€§æµ‹è¯•
            python_results = await self._test_python_compatibility()
            self.test_results.extend(python_results)
            
            # ç»ˆç«¯å…¼å®¹æ€§æµ‹è¯•
            terminal_results = await self._test_terminal_compatibility_extended()
            self.test_results.extend(terminal_results)
            
            # æ“ä½œç³»ç»Ÿå…¼å®¹æ€§æµ‹è¯•
            os_results = await self._test_os_compatibility()
            self.test_results.extend(os_results)
            
            # å­—ç¬¦ç¼–ç å…¼å®¹æ€§æµ‹è¯•
            encoding_results = await self._test_encoding_compatibility()
            self.test_results.extend(encoding_results)
            
        finally:
            await self.teardown_test_environment()
    
    async def _test_python_compatibility(self) -> List[TestResult]:
        """æµ‹è¯•Pythonå…¼å®¹æ€§"""
        results = []
        
        test_name = "python_version_compatibility"
        start_time = time.time()
        
        try:
            compat_result = await self.test_python_version_compatibility()
            
            success = compat_result["overall_compatible"]
            
            results.append(TestResult(
                test_name=test_name,
                success=success,
                response_time=time.time() - start_time,
                message=f"Python {compat_result['python_version']} å…¼å®¹æ€§: {'é€šè¿‡' if success else 'å¤±è´¥'}",
                details=compat_result
            ))
        
        except Exception as e:
            results.append(TestResult(
                test_name=test_name,
                success=False,
                response_time=time.time() - start_time,
                message=f"Pythonå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {str(e)}",
                details={"error": str(e)}
            ))
        
        return results
    
    async def _test_terminal_compatibility_extended(self) -> List[TestResult]:
        """æ‰©å±•ç»ˆç«¯å…¼å®¹æ€§æµ‹è¯•"""
        results = []
        
        try:
            compat_result = await self.test_terminal_compatibility()
            
            for term_config, result in compat_result.items():
                test_name = f"terminal_compatibility_{term_config}"
                
                success = result.get("success", False)
                config = result.get("config", {})
                
                results.append(TestResult(
                    test_name=test_name,
                    success=success,
                    response_time=0,
                    message=f"ç»ˆç«¯ {config.get('TERM', 'unknown')} å…¼å®¹æ€§: {'é€šè¿‡' if success else 'å¤±è´¥'}",
                    details=result
                ))
        
        except Exception as e:
            results.append(TestResult(
                test_name="terminal_compatibility_error",
                success=False,
                response_time=0,
                message=f"ç»ˆç«¯å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {str(e)}",
                details={"error": str(e)}
            ))
        
        return results
    
    async def _test_os_compatibility(self) -> List[TestResult]:
        """æµ‹è¯•æ“ä½œç³»ç»Ÿå…¼å®¹æ€§"""
        results = []
        
        test_name = "os_compatibility"
        start_time = time.time()
        
        try:
            # è·å–ç³»ç»Ÿä¿¡æ¯
            system_info = {
                "platform": platform.platform(),
                "system": platform.system(),
                "release": platform.release(),
                "version": platform.version(),
                "machine": platform.machine(),
                "processor": platform.processor()
            }
            
            # æµ‹è¯•è·¯å¾„å¤„ç†
            path_test_success = True
            try:
                test_path = Path(self.temp_dir) / "test_file.txt"
                test_path.write_text("æµ‹è¯•å†…å®¹")
                content = test_path.read_text()
                test_path.unlink()
                path_test_success = content == "æµ‹è¯•å†…å®¹"
            except Exception:
                path_test_success = False
            
            # æµ‹è¯•è¿›ç¨‹ç®¡ç†
            process_test_success = True
            try:
                proc = subprocess.Popen([sys.executable, "--version"], 
                                      stdout=subprocess.PIPE, 
                                      stderr=subprocess.PIPE)
                stdout, stderr = proc.communicate(timeout=5)
                process_test_success = proc.returncode == 0
            except Exception:
                process_test_success = False
            
            # ç»¼åˆè¯„ä¼°
            success = path_test_success and process_test_success
            
            results.append(TestResult(
                test_name=test_name,
                success=success,
                response_time=time.time() - start_time,
                message=f"æ“ä½œç³»ç»Ÿ {system_info['system']} å…¼å®¹æ€§: {'é€šè¿‡' if success else 'å¤±è´¥'}",
                details={
                    "system_info": system_info,
                    "path_test": path_test_success,
                    "process_test": process_test_success
                }
            ))
        
        except Exception as e:
            results.append(TestResult(
                test_name=test_name,
                success=False,
                response_time=time.time() - start_time,
                message=f"æ“ä½œç³»ç»Ÿå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {str(e)}",
                details={"error": str(e)}
            ))
        
        return results
    
    async def _test_encoding_compatibility(self) -> List[TestResult]:
        """æµ‹è¯•å­—ç¬¦ç¼–ç å…¼å®¹æ€§"""
        results = []
        
        # æµ‹è¯•ä¸åŒçš„å­—ç¬¦ç¼–ç 
        encoding_tests = [
            {"name": "utf8", "text": "ä½ å¥½ä¸–ç•Œ Hello World! ğŸš€"},
            {"name": "ascii", "text": "Hello World"},
            {"name": "emoji", "text": "ğŸ¤– AIåŠ©æ‰‹ ğŸ’¬ å¯¹è¯ âš¡ æ€§èƒ½"},
            {"name": "special_chars", "text": "ç‰¹æ®Šå­—ç¬¦: !@#$%^&*()"},
            {"name": "multiline", "text": "ç¬¬ä¸€è¡Œ\nç¬¬äºŒè¡Œ\nç¬¬ä¸‰è¡Œ"}
        ]
        
        for test in encoding_tests:
            test_name = f"encoding_compatibility_{test['name']}"
            start_time = time.time()
            
            try:
                # æµ‹è¯•CLIèƒ½å¦æ­£ç¡®å¤„ç†è¿™äº›å­—ç¬¦
                result = await self.run_cli_command(
                    "python start_cli.py",
                    [test["text"], "exit"],
                    timeout=10
                )
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç¼–ç é”™è¯¯
                has_encoding_error = (
                    "UnicodeDecodeError" in result["stderr"] or
                    "UnicodeEncodeError" in result["stderr"] or
                    "encoding" in result["stderr"].lower()
                )
                
                success = result["success"] and not has_encoding_error
                
                results.append(TestResult(
                    test_name=test_name,
                    success=success,
                    response_time=time.time() - start_time,
                    message=f"ç¼–ç æµ‹è¯• {test['name']}: {'é€šè¿‡' if success else 'å¤±è´¥'}",
                    details={
                        "test_text": test["text"],
                        "command_result": result,
                        "has_encoding_error": has_encoding_error
                    }
                ))
            
            except Exception as e:
                results.append(TestResult(
                    test_name=test_name,
                    success=False,
                    response_time=time.time() - start_time,
                    message=f"ç¼–ç å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {str(e)}",
                    details={"test_text": test["text"], "error": str(e)}
                ))
        
        return results


# è¿è¡Œæµ‹è¯•çš„ä¸»å‡½æ•°
async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹CLIæ€§èƒ½å’Œå…¼å®¹æ€§æµ‹è¯•")
    print("=" * 60)
    
    # æ€§èƒ½æµ‹è¯•
    performance_suite = CLIPerformanceTestSuite()
    await performance_suite.run_performance_tests()
    performance_report = performance_suite.generate_test_report()
    
    # å…¼å®¹æ€§æµ‹è¯•
    compatibility_suite = CLICompatibilityTestSuite()
    await compatibility_suite.run_compatibility_tests()
    compatibility_report = compatibility_suite.generate_test_report()
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "="*60)
    print("CLIæ€§èƒ½æµ‹è¯•æŠ¥å‘Š:")
    print(f"æ€»æµ‹è¯•æ•°: {performance_report['summary']['total_tests']}")
    print(f"é€šè¿‡: {performance_report['summary']['passed']}")
    print(f"å¤±è´¥: {performance_report['summary']['failed']}")
    print(f"æˆåŠŸç‡: {performance_report['summary']['success_rate']}")
    
    print("\nCLIå…¼å®¹æ€§æµ‹è¯•æŠ¥å‘Š:")
    print(f"æ€»æµ‹è¯•æ•°: {compatibility_report['summary']['total_tests']}")
    print(f"é€šè¿‡: {compatibility_report['summary']['passed']}")
    print(f"å¤±è´¥: {compatibility_report['summary']['failed']}")
    print(f"æˆåŠŸç‡: {compatibility_report['summary']['success_rate']}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    test_reports_dir = Path("test_reports")
    test_reports_dir.mkdir(exist_ok=True)
    
    with open(test_reports_dir / "cli_performance_report.json", "w", encoding="utf-8") as f:
        json.dump(performance_report, f, ensure_ascii=False, indent=2)
    
    with open(test_reports_dir / "cli_compatibility_report.json", "w", encoding="utf-8") as f:
        json.dump(compatibility_report, f, ensure_ascii=False, indent=2)
    
    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° {test_reports_dir}")


if __name__ == "__main__":
    asyncio.run(main())