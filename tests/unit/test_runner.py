#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶è¿è¡Œå™¨
æä¾›ç»Ÿä¸€çš„æµ‹è¯•æ‰§è¡Œã€æŠ¥å‘Šç”Ÿæˆå’Œç»“æœç®¡ç†åŠŸèƒ½
"""
import asyncio
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import asdict
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from tests.unit.base import BaseTestSuite, TestResult
from tests.unit.test_dialog_system import DialogSystemTestSuite
from tests.unit.test_intelligent_routing import IntelligentRoutingTestSuite
from tests.unit.test_system_commands import SystemCommandTestSuite
from tests.unit.test_resource_management import ResourceManagementTestSuite
from tests.e2e.test_integration_flows import EndToEndIntegrationTestSuite


class AutomatedTestRunner:
    """è‡ªåŠ¨åŒ–æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000", output_dir: str = "test_reports"):
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # æ³¨å†Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶
        self.test_suites = {
            "dialog": DialogSystemTestSuite(base_url),
            "routing": IntelligentRoutingTestSuite(base_url),
            "commands": SystemCommandTestSuite(base_url),
            "resources": ResourceManagementTestSuite(base_url),
            "e2e": EndToEndIntegrationTestSuite(base_url)
        }
        
        self.overall_results = {}
        self.start_time = None
        self.end_time = None
    
    async def run_health_check(self) -> bool:
        """è¿è¡Œå¥åº·æ£€æŸ¥"""
        print("ğŸ¥ æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
        
        try:
            # ä½¿ç”¨åŸºç¡€æµ‹è¯•å¥—ä»¶è¿›è¡Œå¥åº·æ£€æŸ¥
            base_suite = BaseTestSuite(self.base_url)
            health_result = await base_suite.get_health_status()
            
            if "error" in health_result:
                print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {health_result['error']}")
                return False
            
            print("âœ… ç³»ç»Ÿå¥åº·æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    async def run_single_suite(self, suite_name: str, suite: BaseTestSuite) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¥—ä»¶"""
        print(f"\nğŸ§ª å¼€å§‹è¿è¡Œæµ‹è¯•å¥—ä»¶: {suite_name}")
        print("=" * 60)
        
        suite_start_time = time.time()
        
        try:
            # æ ¹æ®å¥—ä»¶ç±»å‹è¿è¡Œç›¸åº”çš„æµ‹è¯•æ–¹æ³•
            if hasattr(suite, 'run_dialog_tests'):
                await suite.run_dialog_tests()
            elif hasattr(suite, 'run_routing_tests'):
                await suite.run_routing_tests()
            elif hasattr(suite, 'run_command_tests'):
                await suite.run_command_tests()
            elif hasattr(suite, 'run_resource_tests'):
                await suite.run_resource_tests()
            elif hasattr(suite, 'run_e2e_tests'):
                await suite.run_e2e_tests()
            else:
                # é»˜è®¤è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
                if hasattr(suite, 'dialog_test_cases'):
                    await suite.run_all_tests(suite.dialog_test_cases)
                elif hasattr(suite, 'routing_test_cases'):
                    await suite.run_all_tests(suite.routing_test_cases)
                elif hasattr(suite, 'command_test_cases'):
                    await suite.run_all_tests(suite.command_test_cases)
                elif hasattr(suite, 'resource_test_cases'):
                    await suite.run_all_tests(suite.resource_test_cases)
                elif hasattr(suite, 'e2e_test_cases'):
                    await suite.run_all_tests(suite.e2e_test_cases)
            
            suite_end_time = time.time()
            suite_duration = suite_end_time - suite_start_time
            
            # è·å–æµ‹è¯•æŠ¥å‘Š
            report = suite.generate_test_report()
            report["suite_name"] = suite_name
            report["duration_seconds"] = round(suite_duration, 2)
            report["timestamp"] = time.time()
            
            print(f"âœ… æµ‹è¯•å¥—ä»¶ {suite_name} å®Œæˆ")
            print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {suite_duration:.2f}ç§’")
            
            return report
            
        except Exception as e:
            suite_end_time = time.time()
            suite_duration = suite_end_time - suite_start_time
            
            error_report = {
                "suite_name": suite_name,
                "error": str(e),
                "duration_seconds": round(suite_duration, 2),
                "timestamp": time.time(),
                "summary": {
                    "total_tests": 0,
                    "passed": 0,
                    "failed": 0,
                    "errors": 1,
                    "skipped": 0,
                    "success_rate": "0%"
                }
            }
            
            print(f"âŒ æµ‹è¯•å¥—ä»¶ {suite_name} æ‰§è¡Œå¤±è´¥: {e}")
            return error_report
    
    async def run_all_suites(self, selected_suites: Optional[List[str]] = None) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶"""
        print("ğŸš€ å¼€å§‹AIåŠ©æ‰‹åŠŸèƒ½æµ‹è¯•ä¸å¯¹è¯éªŒè¯")
        print("=" * 80)
        
        self.start_time = time.time()
        
        # ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•å¥—ä»¶
        if selected_suites:
            suites_to_run = {name: suite for name, suite in self.test_suites.items() 
                           if name in selected_suites}
        else:
            suites_to_run = self.test_suites
        
        # è¿è¡Œæ¯ä¸ªæµ‹è¯•å¥—ä»¶
        for suite_name, suite in suites_to_run.items():
            suite_report = await self.run_single_suite(suite_name, suite)
            self.overall_results[suite_name] = suite_report
        
        self.end_time = time.time()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        overall_report = self._generate_overall_report()
        
        return overall_report
    
    def _generate_overall_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        total_duration = self.end_time - self.start_time if self.start_time and self.end_time else 0
        
        # æ±‡æ€»ç»Ÿè®¡
        overall_stats = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "errors": 0,
            "skipped": 0
        }
        
        suite_summaries = {}
        performance_metrics = {
            "total_response_times": [],
            "suite_durations": [],
            "average_response_time": 0,
            "total_execution_time": total_duration
        }
        
        # æ±‡æ€»å„å¥—ä»¶ç»“æœ
        for suite_name, report in self.overall_results.items():
            if "summary" in report:
                summary = report["summary"]
                overall_stats["total_tests"] += summary.get("total_tests", 0)
                overall_stats["passed"] += summary.get("passed", 0)
                overall_stats["failed"] += summary.get("failed", 0)
                overall_stats["errors"] += summary.get("errors", 0)
                overall_stats["skipped"] += summary.get("skipped", 0)
                
                suite_summaries[suite_name] = {
                    "tests": summary.get("total_tests", 0),
                    "success_rate": summary.get("success_rate", "0%"),
                    "duration": report.get("duration_seconds", 0)
                }
                
                # æ€§èƒ½æŒ‡æ ‡
                if "performance" in report:
                    perf = report["performance"]
                    if perf.get("avg_response_time_ms", 0) > 0:
                        performance_metrics["total_response_times"].append(perf["avg_response_time_ms"])
                
                performance_metrics["suite_durations"].append(report.get("duration_seconds", 0))
        
        # è®¡ç®—ç»¼åˆæˆåŠŸç‡
        if overall_stats["total_tests"] > 0:
            success_rate = (overall_stats["passed"] / overall_stats["total_tests"]) * 100
            overall_stats["success_rate"] = f"{success_rate:.1f}%"
        else:
            overall_stats["success_rate"] = "0%"
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        if performance_metrics["total_response_times"]:
            avg_response_time = sum(performance_metrics["total_response_times"]) / len(performance_metrics["total_response_times"])
            performance_metrics["average_response_time"] = round(avg_response_time, 2)
        
        # æµ‹è¯•è´¨é‡è¯„ä¼°
        quality_assessment = self._assess_test_quality(overall_stats, performance_metrics)
        
        overall_report = {
            "test_execution_summary": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.start_time)),
                "total_duration_seconds": round(total_duration, 2),
                "suites_executed": len(self.overall_results),
                "base_url": self.base_url
            },
            "overall_statistics": overall_stats,
            "suite_summaries": suite_summaries,
            "performance_metrics": performance_metrics,
            "quality_assessment": quality_assessment,
            "detailed_results": self.overall_results
        }
        
        return overall_report
    
    def _assess_test_quality(self, stats: Dict[str, Any], performance: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°æµ‹è¯•è´¨é‡"""
        assessment = {
            "overall_health": "UNKNOWN",
            "reliability_score": 0.0,
            "performance_score": 0.0,
            "coverage_score": 0.0,
            "recommendations": []
        }
        
        # å¯é æ€§è¯„åˆ†ï¼ˆåŸºäºæˆåŠŸç‡ï¼‰
        success_rate_str = stats.get("success_rate", "0%")
        success_rate = float(success_rate_str.replace("%", ""))
        
        if success_rate >= 95:
            assessment["reliability_score"] = 1.0
            assessment["overall_health"] = "EXCELLENT"
        elif success_rate >= 85:
            assessment["reliability_score"] = 0.8
            assessment["overall_health"] = "GOOD"
        elif success_rate >= 70:
            assessment["reliability_score"] = 0.6
            assessment["overall_health"] = "FAIR"
        else:
            assessment["reliability_score"] = 0.3
            assessment["overall_health"] = "POOR"
        
        # æ€§èƒ½è¯„åˆ†ï¼ˆåŸºäºå“åº”æ—¶é—´ï¼‰
        avg_response_time = performance.get("average_response_time", 0)
        if avg_response_time <= 1000:  # 1ç§’ä»¥å†…
            assessment["performance_score"] = 1.0
        elif avg_response_time <= 3000:  # 3ç§’ä»¥å†…
            assessment["performance_score"] = 0.8
        elif avg_response_time <= 5000:  # 5ç§’ä»¥å†…
            assessment["performance_score"] = 0.6
        else:
            assessment["performance_score"] = 0.3
        
        # è¦†ç›–ç‡è¯„åˆ†ï¼ˆåŸºäºæµ‹è¯•å¥—ä»¶æ•°é‡å’Œæµ‹è¯•ç”¨ä¾‹æ•°é‡ï¼‰
        total_tests = stats.get("total_tests", 0)
        suites_count = len(self.overall_results)
        
        if total_tests >= 20 and suites_count >= 4:
            assessment["coverage_score"] = 1.0
        elif total_tests >= 15 and suites_count >= 3:
            assessment["coverage_score"] = 0.8
        elif total_tests >= 10 and suites_count >= 2:
            assessment["coverage_score"] = 0.6
        else:
            assessment["coverage_score"] = 0.4
        
        # ç”Ÿæˆå»ºè®®
        if assessment["reliability_score"] < 0.8:
            assessment["recommendations"].append("å»ºè®®ä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ï¼Œæé«˜ç³»ç»Ÿç¨³å®šæ€§")
        
        if assessment["performance_score"] < 0.8:
            assessment["recommendations"].append("å»ºè®®ä¼˜åŒ–å“åº”æ—¶é—´ï¼Œç›®æ ‡å“åº”æ—¶é—´åº”åœ¨3ç§’ä»¥å†…")
        
        if assessment["coverage_score"] < 0.8:
            assessment["recommendations"].append("å»ºè®®å¢åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹ï¼Œæé«˜æµ‹è¯•è¦†ç›–ç‡")
        
        if stats.get("errors", 0) > 0:
            assessment["recommendations"].append("å»ºè®®æ£€æŸ¥å¹¶ä¿®å¤æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯")
        
        return assessment
    
    def save_report(self, report: Dict[str, Any], format_type: str = "json") -> str:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        if format_type == "json":
            filename = f"test_report_{timestamp}.json"
            filepath = self.output_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
        
        elif format_type == "html":
            filename = f"test_report_{timestamp}.html"
            filepath = self.output_dir / filename
            
            html_content = self._generate_html_report(report)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŠ¥å‘Šæ ¼å¼: {format_type}")
        
        return str(filepath)
    
    def _generate_html_report(self, report: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
        html_template = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIåŠ©æ‰‹åŠŸèƒ½æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
        .header { text-align: center; margin-bottom: 30px; }
        .summary { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin-bottom: 30px; }
        .metric-card { background: #f8f9fa; padding: 15px; border-radius: 6px; text-align: center; }
        .metric-value { font-size: 2em; font-weight: bold; color: #007bff; }
        .metric-label { color: #666; margin-top: 5px; }
        .suite-results { margin-bottom: 30px; }
        .suite { margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 6px; }
        .suite-header { font-weight: bold; margin-bottom: 10px; }
        .success { color: #28a745; }
        .failure { color: #dc3545; }
        .warning { color: #ffc107; }
        .recommendations { background: #e7f3ff; padding: 15px; border-radius: 6px; margin-top: 20px; }
        .quality-indicator { display: inline-block; padding: 4px 8px; border-radius: 4px; color: white; font-weight: bold; }
        .excellent { background-color: #28a745; }
        .good { background-color: #17a2b8; }
        .fair { background-color: #ffc107; }
        .poor { background-color: #dc3545; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¤– AIåŠ©æ‰‹åŠŸèƒ½æµ‹è¯•æŠ¥å‘Š</h1>
            <p>æµ‹è¯•æ—¶é—´: {timestamp}</p>
            <p>æ€»æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’</p>
        </div>
        
        <div class="summary">
            <div class="metric-card">
                <div class="metric-value">{total_tests}</div>
                <div class="metric-label">æ€»æµ‹è¯•æ•°</div>
            </div>
            <div class="metric-card">
                <div class="metric-value success">{passed}</div>
                <div class="metric-label">é€šè¿‡</div>
            </div>
            <div class="metric-card">
                <div class="metric-value failure">{failed}</div>
                <div class="metric-label">å¤±è´¥</div>
            </div>
            <div class="metric-card">
                <div class="metric-value warning">{errors}</div>
                <div class="metric-label">é”™è¯¯</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{success_rate}</div>
                <div class="metric-label">æˆåŠŸç‡</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{avg_response_time:.0f}ms</div>
                <div class="metric-label">å¹³å‡å“åº”æ—¶é—´</div>
            </div>
        </div>
        
        <div class="quality-section">
            <h2>è´¨é‡è¯„ä¼°</h2>
            <p>ç³»ç»Ÿå¥åº·çŠ¶å†µ: <span class="quality-indicator {health_class}">{health_status}</span></p>
            <p>å¯é æ€§è¯„åˆ†: {reliability_score:.1%}</p>
            <p>æ€§èƒ½è¯„åˆ†: {performance_score:.1%}</p>
            <p>è¦†ç›–ç‡è¯„åˆ†: {coverage_score:.1%}</p>
        </div>
        
        <div class="suite-results">
            <h2>æµ‹è¯•å¥—ä»¶ç»“æœ</h2>
            {suite_details}
        </div>
        
        {recommendations_section}
    </div>
</body>
</html>
        """
        
        # å‡†å¤‡æ•°æ®
        execution_summary = report.get("test_execution_summary", {})
        stats = report.get("overall_statistics", {})
        performance = report.get("performance_metrics", {})
        quality = report.get("quality_assessment", {})
        suite_summaries = report.get("suite_summaries", {})
        
        # ç”Ÿæˆå¥—ä»¶è¯¦æƒ…
        suite_details = ""
        for suite_name, summary in suite_summaries.items():
            suite_details += f"""
            <div class="suite">
                <div class="suite-header">{suite_name.upper()} æµ‹è¯•å¥—ä»¶</div>
                <p>æµ‹è¯•æ•°é‡: {summary.get('tests', 0)}</p>
                <p>æˆåŠŸç‡: {summary.get('success_rate', '0%')}</p>
                <p>æ‰§è¡Œæ—¶é—´: {summary.get('duration', 0):.2f}ç§’</p>
            </div>
            """
        
        # ç”Ÿæˆå»ºè®®éƒ¨åˆ†
        recommendations = quality.get("recommendations", [])
        if recommendations:
            recommendations_section = f"""
            <div class="recommendations">
                <h3>æ”¹è¿›å»ºè®®</h3>
                <ul>
                    {''.join(f'<li>{rec}</li>' for rec in recommendations)}
                </ul>
            </div>
            """
        else:
            recommendations_section = ""
        
        # ç¡®å®šå¥åº·çŠ¶å†µæ ·å¼
        health_status = quality.get("overall_health", "UNKNOWN")
        health_class = health_status.lower()
        
        return html_template.format(
            timestamp=execution_summary.get("timestamp", "æœªçŸ¥"),
            duration=execution_summary.get("total_duration_seconds", 0),
            total_tests=stats.get("total_tests", 0),
            passed=stats.get("passed", 0),
            failed=stats.get("failed", 0),
            errors=stats.get("errors", 0),
            success_rate=stats.get("success_rate", "0%"),
            avg_response_time=performance.get("average_response_time", 0),
            health_status=health_status,
            health_class=health_class,
            reliability_score=quality.get("reliability_score", 0),
            performance_score=quality.get("performance_score", 0),
            coverage_score=quality.get("coverage_score", 0),
            suite_details=suite_details,
            recommendations_section=recommendations_section
        )
    
    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“Š AIåŠ©æ‰‹åŠŸèƒ½æµ‹è¯•å®Œæˆæ‘˜è¦")
        print("=" * 80)
        
        execution_summary = report.get("test_execution_summary", {})
        stats = report.get("overall_statistics", {})
        performance = report.get("performance_metrics", {})
        quality = report.get("quality_assessment", {})
        
        print(f"ğŸ• æµ‹è¯•æ—¶é—´: {execution_summary.get('timestamp', 'æœªçŸ¥')}")
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {execution_summary.get('total_duration_seconds', 0):.2f}ç§’")
        print(f"ğŸ§ª æµ‹è¯•å¥—ä»¶æ•°: {execution_summary.get('suites_executed', 0)}")
        print(f"ğŸ“ˆ æ€»æµ‹è¯•æ•°: {stats.get('total_tests', 0)}")
        print(f"âœ… é€šè¿‡: {stats.get('passed', 0)}")
        print(f"âŒ å¤±è´¥: {stats.get('failed', 0)}")
        print(f"ğŸ’¥ é”™è¯¯: {stats.get('errors', 0)}")
        print(f"â­ï¸ è·³è¿‡: {stats.get('skipped', 0)}")
        print(f"ğŸ“Š æˆåŠŸç‡: {stats.get('success_rate', '0%')}")
        print(f"âš¡ å¹³å‡å“åº”æ—¶é—´: {performance.get('average_response_time', 0):.0f}ms")
        
        # è´¨é‡è¯„ä¼°
        print(f"\nğŸ¥ ç³»ç»Ÿå¥åº·çŠ¶å†µ: {quality.get('overall_health', 'UNKNOWN')}")
        print(f"ğŸ”’ å¯é æ€§è¯„åˆ†: {quality.get('reliability_score', 0):.1%}")
        print(f"âš¡ æ€§èƒ½è¯„åˆ†: {quality.get('performance_score', 0):.1%}")
        print(f"ğŸ“‹ è¦†ç›–ç‡è¯„åˆ†: {quality.get('coverage_score', 0):.1%}")
        
        # å»ºè®®
        recommendations = quality.get("recommendations", [])
        if recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print("=" * 80)


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AIåŠ©æ‰‹è‡ªåŠ¨åŒ–æµ‹è¯•è¿è¡Œå™¨")
    parser.add_argument("--url", default="http://localhost:8000", help="æœåŠ¡å™¨URL")
    parser.add_argument("--suites", nargs="+", choices=["dialog", "routing", "commands", "resources", "e2e"],
                       help="æŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•å¥—ä»¶")
    parser.add_argument("--output", default="test_reports", help="æŠ¥å‘Šè¾“å‡ºç›®å½•")
    parser.add_argument("--format", choices=["json", "html"], default="json", help="æŠ¥å‘Šæ ¼å¼")
    parser.add_argument("--skip-health-check", action="store_true", help="è·³è¿‡å¥åº·æ£€æŸ¥")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = AutomatedTestRunner(base_url=args.url, output_dir=args.output)
    
    try:
        # å¥åº·æ£€æŸ¥
        if not args.skip_health_check:
            health_ok = await runner.run_health_check()
            if not health_ok:
                print("âŒ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥æœåŠ¡çŠ¶æ€åé‡è¯•")
                return 1
        
        # è¿è¡Œæµ‹è¯•
        report = await runner.run_all_suites(args.suites)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = runner.save_report(report, args.format)
        print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ‰“å°æ‘˜è¦
        runner.print_summary(report)
        
        # æ ¹æ®æµ‹è¯•ç»“æœç¡®å®šé€€å‡ºç 
        stats = report.get("overall_statistics", {})
        if stats.get("failed", 0) > 0 or stats.get("errors", 0) > 0:
            return 1
        else:
            return 0
            
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿è¡Œå¼‚å¸¸: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)