#!/usr/bin/env python3
"""
å¯¹è¯ç³»ç»ŸåŠŸèƒ½æµ‹è¯•å¥—ä»¶
æµ‹è¯•åŸºç¡€å¯¹è¯ã€å¤æ‚æ¨ç†ã€ä¸Šä¸‹æ–‡è®°å¿†ã€å¤šè½®å¯¹è¯ã€æµå¼å“åº”ç­‰åŠŸèƒ½
"""
import asyncio
import time
from typing import List, Dict, Any
from tests.base import BaseTestSuite, TestCase, TestResult, TestMetrics, PerformanceTestMixin


class DialogSystemTestSuite(BaseTestSuite, PerformanceTestMixin):
    """å¯¹è¯ç³»ç»Ÿæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        super().__init__(base_url)
        self.dialog_test_cases = self._create_dialog_test_cases()
    
    def _create_dialog_test_cases(self) -> List[TestCase]:
        """åˆ›å»ºå¯¹è¯æµ‹è¯•ç”¨ä¾‹"""
        return [
            # åŸºç¡€å¯¹è¯æµ‹è¯•
            TestCase(
                name="basic_greeting",
                description="æµ‹è¯•åŸºç¡€é—®å€™å¯¹è¯",
                input_data={"message": "ä½ å¥½", "expected_model": "local", "max_time": 500},
                expected_output={"should_contain": ["ä½ å¥½", "åŠ©æ‰‹"], "min_length": 10},
                category="åŸºç¡€å¯¹è¯",
                priority="HIGH"
            ),
            TestCase(
                name="self_introduction",
                description="æµ‹è¯•è‡ªæˆ‘ä»‹ç»",
                input_data={"message": "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ", "expected_model": "auto", "max_time": 800},
                expected_output={"should_contain": ["åŠ©æ‰‹", "å¸®åŠ©"], "min_length": 20},
                category="åŸºç¡€å¯¹è¯",
                priority="HIGH"
            ),
            TestCase(
                name="system_capabilities",
                description="æµ‹è¯•ç³»ç»Ÿèƒ½åŠ›ä»‹ç»",
                input_data={"message": "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", "expected_model": "auto", "max_time": 1000},
                expected_output={"should_contain": ["AI", "åŠ©æ‰‹"], "min_length": 30},
                category="åŸºç¡€å¯¹è¯",
                priority="MEDIUM"
            ),
            
            # å¤æ‚æ¨ç†æµ‹è¯•
            TestCase(
                name="complex_analysis",
                description="æµ‹è¯•å¤æ‚åˆ†æä»»åŠ¡",
                input_data={"message": "åˆ†æä¸­ç¾è´¸æ˜“æˆ˜å¯¹å…¨çƒç»æµçš„å½±å“", "expected_model": "cloud", "max_time": 5000},
                expected_output={"should_contain": ["è´¸æ˜“", "ç»æµ", "å½±å“"], "min_length": 100},
                category="å¤æ‚æ¨ç†",
                priority="HIGH"
            ),
            TestCase(
                name="technical_design",
                description="æµ‹è¯•æŠ€æœ¯è®¾è®¡ä»»åŠ¡",
                input_data={"message": "è®¾è®¡ä¸€ä¸ªå¾®æœåŠ¡æ¶æ„æ–¹æ¡ˆ", "expected_model": "cloud", "max_time": 8000},
                expected_output={"should_contain": ["å¾®æœåŠ¡", "æ¶æ„"], "min_length": 80},
                category="å¤æ‚æ¨ç†",
                priority="MEDIUM"
            ),
            
            # å®æ—¶ä¿¡æ¯æµ‹è¯•
            TestCase(
                name="weather_query",
                description="æµ‹è¯•å¤©æ°”æŸ¥è¯¢åŠŸèƒ½",
                input_data={"message": "ä»Šå¤©åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", "expected_plugin": "weather", "max_time": 3000},
                expected_output={"should_contain": ["å¤©æ°”", "åŒ—äº¬"], "min_length": 20},
                category="å®æ—¶ä¿¡æ¯",
                priority="HIGH"
            ),
            TestCase(
                name="current_tech_trends",
                description="æµ‹è¯•æœ€æ–°æŠ€æœ¯è¶‹åŠ¿æŸ¥è¯¢",
                input_data={"message": "æœ€æ–°çš„AIæŠ€æœ¯å‘å±•", "expected_model": "cloud", "max_time": 6000},
                expected_output={"should_contain": ["AI", "æŠ€æœ¯"], "min_length": 50},
                category="å®æ—¶ä¿¡æ¯",
                priority="MEDIUM"
            ),
            
            # ä¸Šä¸‹æ–‡è®°å¿†æµ‹è¯•
            TestCase(
                name="context_memory",
                description="æµ‹è¯•ä¸Šä¸‹æ–‡è®°å¿†",
                input_data={"message": "æˆ‘åˆšæ‰é—®äº†ä»€ä¹ˆï¼Ÿ", "requires_context": True, "max_time": 1000},
                expected_output={"should_reference_previous": True, "min_length": 15},
                category="ä¸Šä¸‹æ–‡è®°å¿†",
                priority="HIGH"
            ),
            
            # å¤šè½®å¯¹è¯æµ‹è¯•
            TestCase(
                name="multi_turn_dialog",
                description="æµ‹è¯•å¤šè½®å¯¹è¯è¿è´¯æ€§",
                input_data={"turns": [
                    "æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹",
                    "ä»å“ªé‡Œå¼€å§‹æ¯”è¾ƒå¥½ï¼Ÿ",
                    "éœ€è¦ä»€ä¹ˆåŸºç¡€çŸ¥è¯†ï¼Ÿ"
                ], "max_time": 2000},
                expected_output={"coherent": True, "contextual": True},
                category="å¤šè½®å¯¹è¯",
                priority="HIGH"
            ),
            
            # æµå¼å“åº”æµ‹è¯•
            TestCase(
                name="stream_response",
                description="æµ‹è¯•æµå¼å“åº”",
                input_data={"message": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—", "stream": True, "max_time": 5000},
                expected_output={"should_stream": True, "min_chunks": 3},
                category="æµå¼å“åº”",
                priority="MEDIUM"
            )
        ]
    
    async def run_test_case(self, test_case: TestCase):
        """è¿è¡Œå•ä¸ªå¯¹è¯æµ‹è¯•ç”¨ä¾‹"""
        try:
            if test_case.category == "åŸºç¡€å¯¹è¯":
                await self._test_basic_dialog(test_case)
            elif test_case.category == "å¤æ‚æ¨ç†":
                await self._test_complex_reasoning(test_case)
            elif test_case.category == "å®æ—¶ä¿¡æ¯":
                await self._test_realtime_info(test_case)
            elif test_case.category == "ä¸Šä¸‹æ–‡è®°å¿†":
                await self._test_context_memory(test_case)
            elif test_case.category == "å¤šè½®å¯¹è¯":
                await self._test_multi_turn_dialog(test_case)
            elif test_case.category == "æµå¼å“åº”":
                await self._test_stream_response(test_case)
            else:
                self.record_test_result(test_case, TestResult.SKIP, 
                                      details={"error": f"æœªçŸ¥æµ‹è¯•ç±»åˆ«: {test_case.category}"})
                
        except Exception as e:
            self.record_test_result(test_case, TestResult.ERROR,
                                  details={"error": f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"})
    
    async def _test_basic_dialog(self, test_case: TestCase):
        """æµ‹è¯•åŸºç¡€å¯¹è¯"""
        message = test_case.input_data["message"]
        max_time = test_case.input_data["max_time"]
        
        # å‘é€è¯·æ±‚
        result = await self.send_chat_request(message)
        
        # æ£€æŸ¥å“åº”
        if "error" in result:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": result["error"]})
            return
        
        # éªŒè¯å†…å®¹
        content = result.get("content", "")
        expected = test_case.expected_output
        
        # æ£€æŸ¥å¿…é¡»åŒ…å«çš„å…³é”®è¯
        missing_keywords = []
        for keyword in expected.get("should_contain", []):
            if keyword not in content:
                missing_keywords.append(keyword)
        
        # æ£€æŸ¥æœ€å°é•¿åº¦
        min_length = expected.get("min_length", 0)
        if len(content) < min_length:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": f"å›å¤é•¿åº¦ä¸è¶³: {len(content)} < {min_length}"})
            return
        
        if missing_keywords:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": f"ç¼ºå°‘å…³é”®è¯: {missing_keywords}"})
            return
        
        # æ£€æŸ¥å“åº”æ—¶é—´
        response_time = result.get("_response_time_ms", 0)
        performance_ok = self.assert_performance(response_time, max_time, test_case.name)
        
        # è®°å½•æˆåŠŸç»“æœ
        metrics = TestMetrics(
            response_time_ms=response_time,
            token_count=result.get("token_count", 0),
            model_used=result.get("model_used", "unknown"),
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=True
        )
        
        self.record_test_result(test_case, TestResult.PASS, metrics=metrics,
                              details={"content_length": len(content), "performance_ok": performance_ok})
    
    async def _test_complex_reasoning(self, test_case: TestCase):
        """æµ‹è¯•å¤æ‚æ¨ç†"""
        message = test_case.input_data["message"]
        max_time = test_case.input_data["max_time"]
        
        result = await self.send_chat_request(message, max_tokens=2000)
        
        if "error" in result:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": result["error"]})
            return
        
        content = result.get("content", "")
        expected = test_case.expected_output
        
        # éªŒè¯å¤æ‚æ¨ç†å†…å®¹è´¨é‡
        missing_keywords = []
        for keyword in expected.get("should_contain", []):
            if keyword not in content:
                missing_keywords.append(keyword)
        
        min_length = expected.get("min_length", 0)
        if len(content) < min_length:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": f"æ¨ç†å†…å®¹é•¿åº¦ä¸è¶³: {len(content)} < {min_length}"})
            return
        
        if missing_keywords:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": f"æ¨ç†å†…å®¹ç¼ºå°‘å…³é”®æ¦‚å¿µ: {missing_keywords}"})
            return
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†åˆé€‚çš„æ¨¡å‹
        model_used = result.get("model_used", "")
        expected_model = test_case.input_data.get("expected_model")
        if expected_model == "cloud" and "cloud" not in model_used.lower():
            self.logger.warning(f"é¢„æœŸä½¿ç”¨äº‘ç«¯æ¨¡å‹ï¼Œå®é™…ä½¿ç”¨: {model_used}")
        
        response_time = result.get("_response_time_ms", 0)
        performance_ok = self.assert_performance(response_time, max_time, test_case.name)
        
        metrics = TestMetrics(
            response_time_ms=response_time,
            token_count=result.get("token_count", 0),
            model_used=model_used,
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=True
        )
        
        self.record_test_result(test_case, TestResult.PASS, metrics=metrics,
                              details={"reasoning_quality": "satisfactory", "performance_ok": performance_ok})
    
    async def _test_realtime_info(self, test_case: TestCase):
        """æµ‹è¯•å®æ—¶ä¿¡æ¯æŸ¥è¯¢"""
        message = test_case.input_data["message"]
        max_time = test_case.input_data["max_time"]
        
        result = await self.send_chat_request(message)
        
        if "error" in result:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": result["error"]})
            return
        
        content = result.get("content", "")
        expected = test_case.expected_output
        
        # éªŒè¯å®æ—¶ä¿¡æ¯å†…å®¹
        missing_keywords = []
        for keyword in expected.get("should_contain", []):
            if keyword not in content:
                missing_keywords.append(keyword)
        
        if missing_keywords:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": f"å®æ—¶ä¿¡æ¯ç¼ºå°‘å…³é”®è¯: {missing_keywords}"})
            return
        
        # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†æ’ä»¶
        expected_plugin = test_case.input_data.get("expected_plugin")
        if expected_plugin:
            # è¿™é‡Œå¯ä»¥æ£€æŸ¥æ—¥å¿—æˆ–å“åº”å…ƒæ•°æ®æ¥ç¡®è®¤æ’ä»¶ä½¿ç”¨
            self.logger.info(f"é¢„æœŸä½¿ç”¨æ’ä»¶: {expected_plugin}")
        
        response_time = result.get("_response_time_ms", 0)
        performance_ok = self.assert_performance(response_time, max_time, test_case.name)
        
        metrics = TestMetrics(
            response_time_ms=response_time,
            token_count=result.get("token_count", 0),
            model_used=result.get("model_used", "unknown"),
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=True
        )
        
        self.record_test_result(test_case, TestResult.PASS, metrics=metrics,
                              details={"info_type": "realtime", "performance_ok": performance_ok})
    
    async def _test_context_memory(self, test_case: TestCase):
        """æµ‹è¯•ä¸Šä¸‹æ–‡è®°å¿†"""
        # é¦–å…ˆå‘é€ä¸€ä¸ªé—®é¢˜æ¥å»ºç«‹ä¸Šä¸‹æ–‡
        context_message = "æˆ‘æƒ³äº†è§£æœºå™¨å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†"
        await self.send_chat_request(context_message)
        
        # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿ä¸Šä¸‹æ–‡è¢«ä¿å­˜
        await asyncio.sleep(1)
        
        # ç„¶åå‘é€éœ€è¦ä¸Šä¸‹æ–‡çš„é—®é¢˜
        message = test_case.input_data["message"]
        max_time = test_case.input_data["max_time"]
        
        result = await self.send_chat_request(message)
        
        if "error" in result:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": result["error"]})
            return
        
        content = result.get("content", "")
        
        # æ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†ä¹‹å‰çš„ä¸Šä¸‹æ–‡
        context_indicators = ["åˆšæ‰", "ä¹‹å‰", "ä¸Šé¢", "æœºå™¨å­¦ä¹ ", "åˆšåˆš"]
        has_context_reference = any(indicator in content for indicator in context_indicators)
        
        if not has_context_reference:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": "æœªèƒ½æ­£ç¡®å¼•ç”¨ä¸Šä¸‹æ–‡"})
            return
        
        response_time = result.get("_response_time_ms", 0)
        performance_ok = self.assert_performance(response_time, max_time, test_case.name)
        
        metrics = TestMetrics(
            response_time_ms=response_time,
            token_count=result.get("token_count", 0),
            model_used=result.get("model_used", "unknown"),
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=True
        )
        
        self.record_test_result(test_case, TestResult.PASS, metrics=metrics,
                              details={"context_referenced": True, "performance_ok": performance_ok})
    
    async def _test_multi_turn_dialog(self, test_case: TestCase):
        """æµ‹è¯•å¤šè½®å¯¹è¯"""
        turns = test_case.input_data["turns"]
        max_time = test_case.input_data["max_time"]
        
        responses = []
        total_time = 0
        
        for i, turn in enumerate(turns, 1):
            result = await self.send_chat_request(turn)
            
            if "error" in result:
                self.record_test_result(test_case, TestResult.FAIL,
                                      details={"error": f"ç¬¬{i}è½®å¯¹è¯å¤±è´¥: {result['error']}"})
                return
            
            responses.append(result.get("content", ""))
            total_time += result.get("_response_time_ms", 0)
            
            # è½®æ¬¡é—´éš”
            await asyncio.sleep(0.5)
        
        # æ£€æŸ¥å¯¹è¯è¿è´¯æ€§
        coherent = True
        for i in range(1, len(responses)):
            # ç®€å•æ£€æŸ¥ï¼šåç»­å›å¤åº”è¯¥ä¸ä¸»é¢˜ç›¸å…³
            if "Python" in turns[0] and "Python" not in responses[i]:
                coherent = False
                break
        
        performance_ok = self.assert_performance(total_time, max_time * len(turns), test_case.name)
        
        if not coherent:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": "å¤šè½®å¯¹è¯ç¼ºä¹è¿è´¯æ€§"})
            return
        
        metrics = TestMetrics(
            response_time_ms=int(total_time / len(turns)),  # å¹³å‡å“åº”æ—¶é—´
            token_count=sum(len(r.split()) for r in responses),
            model_used="multi-turn",
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=True
        )
        
        self.record_test_result(test_case, TestResult.PASS, metrics=metrics,
                              details={"turns": len(turns), "coherent": coherent, "performance_ok": performance_ok})
    
    async def _test_stream_response(self, test_case: TestCase):
        """æµ‹è¯•æµå¼å“åº”"""
        message = test_case.input_data["message"]
        max_time = test_case.input_data["max_time"]
        
        result = await self.send_stream_request(message)
        
        if "error" in result:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": result["error"]})
            return
        
        chunk_count = result.get("chunk_count", 0)
        min_chunks = test_case.expected_output.get("min_chunks", 1)
        
        if chunk_count < min_chunks:
            self.record_test_result(test_case, TestResult.FAIL,
                                  details={"error": f"æµå¼å—æ•°ä¸è¶³: {chunk_count} < {min_chunks}"})
            return
        
        response_time = result.get("_response_time_ms", 0)
        performance_ok = self.assert_performance(response_time, max_time, test_case.name)
        
        metrics = TestMetrics(
            response_time_ms=response_time,
            token_count=len(result.get("content", "").split()),
            model_used="stream",
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=True
        )
        
        self.record_test_result(test_case, TestResult.PASS, metrics=metrics,
                              details={"chunks": chunk_count, "stream_ok": True, "performance_ok": performance_ok})
    
    async def run_dialog_tests(self):
        """è¿è¡Œæ‰€æœ‰å¯¹è¯æµ‹è¯•"""
        self.logger.info("ğŸ—£ï¸ å¼€å§‹å¯¹è¯ç³»ç»ŸåŠŸèƒ½æµ‹è¯•")
        await self.run_all_tests(self.dialog_test_cases)


async def main():
    """ä¸»å‡½æ•°"""
    test_suite = DialogSystemTestSuite()
    
    try:
        await test_suite.run_dialog_tests()
    except KeyboardInterrupt:
        test_suite.logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        test_suite.logger.error(f"æµ‹è¯•è¿è¡Œå¼‚å¸¸: {e}")


if __name__ == "__main__":
    asyncio.run(main())