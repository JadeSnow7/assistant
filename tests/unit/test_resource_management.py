#!/usr/bin/env python3
"""
èµ„æºç®¡ç†åŠŸèƒ½æµ‹è¯•å¥—ä»¶ - ç¬¬ä¸€éƒ¨åˆ†
æµ‹è¯•ç³»ç»Ÿç›‘æ§ã€æ€§èƒ½ä¼˜åŒ–ã€èµ„æºåˆ†é…ç­‰åŠŸèƒ½
"""
import asyncio
import time
import psutil
import json
from typing import List, Dict, Any, Optional
from tests.base import BaseTestSuite, TestCase, TestResult, TestMetrics, PerformanceTestMixin


class ResourceManagementTestSuite(BaseTestSuite, PerformanceTestMixin):
    """èµ„æºç®¡ç†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        super().__init__(base_url)
        self.resource_test_cases = self._create_resource_test_cases()
        self.baseline_metrics = {}
    
    def _create_resource_test_cases(self) -> List[TestCase]:
        """åˆ›å»ºèµ„æºç®¡ç†æµ‹è¯•ç”¨ä¾‹"""
        return [
            # ç³»ç»Ÿèµ„æºç›‘æ§æµ‹è¯•
            TestCase(
                name="cpu_monitoring_accuracy",
                description="CPUä½¿ç”¨ç‡ç›‘æ§ç²¾åº¦æµ‹è¯•",
                input_data={
                    "message": "æ˜¾ç¤ºCPUä½¿ç”¨ç‡è¯¦ç»†ä¿¡æ¯",
                    "resource_type": "cpu",
                    "accuracy_threshold": 5.0,
                    "max_time": 2000
                },
                expected_output={"should_contain": ["CPU", "ä½¿ç”¨ç‡", "%"], "accuracy_requirement": True},
                category="ç³»ç»Ÿç›‘æ§",
                priority="HIGH"
            ),
            TestCase(
                name="memory_monitoring_accuracy", 
                description="å†…å­˜ä½¿ç”¨ç‡ç›‘æ§ç²¾åº¦æµ‹è¯•",
                input_data={
                    "message": "æŸ¥çœ‹å†…å­˜ä½¿ç”¨è¯¦æƒ…",
                    "resource_type": "memory",
                    "accuracy_threshold": 2.0,
                    "max_time": 2000
                },
                expected_output={"should_contain": ["å†…å­˜", "ä½¿ç”¨", "MB"], "accuracy_requirement": True},
                category="ç³»ç»Ÿç›‘æ§",
                priority="HIGH"
            ),
            # èµ„æºç®¡ç†å†³ç­–æµ‹è¯•
            TestCase(
                name="high_cpu_load_response",
                description="é«˜CPUè´Ÿè½½å“åº”æµ‹è¯•",
                input_data={
                    "message": "ç³»ç»ŸCPUä½¿ç”¨ç‡å¾ˆé«˜ï¼Œæ€ä¹ˆåŠï¼Ÿ",
                    "simulate_condition": {"cpu_usage": 85},
                    "expected_action": "route_to_cloud",
                    "max_time": 3000
                },
                expected_output={"should_suggest_optimization": True, "load_balancing": True},
                category="èµ„æºç®¡ç†å†³ç­–",
                priority="HIGH"
            ),
            # æ€§èƒ½ä¼˜åŒ–æµ‹è¯•
            TestCase(
                name="concurrent_request_handling",
                description="å¹¶å‘è¯·æ±‚å¤„ç†ä¼˜åŒ–æµ‹è¯•",
                input_data={
                    "concurrent_requests": 3,
                    "request_type": "mixed_complexity",
                    "max_time": 6000
                },
                expected_output={"all_requests_handled": True, "no_resource_exhaustion": True},
                category="æ€§èƒ½ä¼˜åŒ–",
                priority="HIGH"
            )
        ]
    
    async def setup(self):
        """æµ‹è¯•å¥—ä»¶åˆå§‹åŒ–"""
        await super().setup()
        self.baseline_metrics = await self._get_system_baseline()
        self.logger.info(f"ğŸ“Š åŸºçº¿ç³»ç»ŸæŒ‡æ ‡: {self.baseline_metrics}")
    
    async def _get_system_baseline(self) -> Dict[str, float]:
        """è·å–ç³»ç»ŸåŸºçº¿æŒ‡æ ‡"""
        try:
            return {
                "cpu_percent": psutil.cpu_percent(interval=1),
                "memory_percent": psutil.virtual_memory().percent,
                "disk_usage_percent": psutil.disk_usage('/').percent
            }
        except Exception as e:
            self.logger.warning(f"è·å–ç³»ç»ŸåŸºçº¿å¤±è´¥: {e}")
            return {}
    
    async def run_test_case(self, test_case: TestCase):
        """è¿è¡Œå•ä¸ªèµ„æºç®¡ç†æµ‹è¯•ç”¨ä¾‹"""
        try:
            if test_case.category == "ç³»ç»Ÿç›‘æ§":
                await self._test_system_monitoring(test_case)
            elif test_case.category == "èµ„æºç®¡ç†å†³ç­–":
                await self._test_resource_management_decision(test_case)
            elif test_case.category == "æ€§èƒ½ä¼˜åŒ–":
                await self._test_performance_optimization(test_case)
            else:
                self.record_test_result(test_case, TestResult.SKIP,
                                      details={"error": f"æœªçŸ¥èµ„æºæµ‹è¯•ç±»åˆ«: {test_case.category}"})
        except Exception as e:
            self.record_test_result(test_case, TestResult.ERROR,
                                  details={"error": f"èµ„æºç®¡ç†æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"})
    
    async def _test_system_monitoring(self, test_case: TestCase):
        """æµ‹è¯•ç³»ç»Ÿç›‘æ§åŠŸèƒ½"""
        message = test_case.input_data["message"]
        resource_type = test_case.input_data["resource_type"]
        max_time = test_case.input_data["max_time"]
        
        actual_metrics = await self._get_actual_system_metrics(resource_type)
        result = await self.send_chat_request(message)
        
        if "error" in result:
            self.record_test_result(test_case, TestResult.FAIL, details={"error": result["error"]})
            return
        
        content = result.get("content", "")
        response_time = result.get("_response_time_ms", 0)
        
        # æ£€æŸ¥å¿…è¦ä¿¡æ¯åŒ…å«
        expected = test_case.expected_output
        missing_keywords = []
        for keyword in expected.get("should_contain", []):
            if keyword not in content:
                missing_keywords.append(keyword)
        
        performance_ok = self.assert_performance(response_time, max_time, test_case.name)
        success = len(missing_keywords) == 0 and performance_ok
        
        metrics = TestMetrics(
            response_time_ms=response_time,
            token_count=result.get("token_count", 0),
            model_used=result.get("model_used", ""),
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=success
        )
        
        test_result = TestResult.PASS if success else TestResult.FAIL
        self.record_test_result(test_case, test_result, metrics=metrics,
                              details={"missing_keywords": missing_keywords, "performance_ok": performance_ok})
    
    async def _get_actual_system_metrics(self, resource_type: str) -> Dict[str, float]:
        """è·å–å®é™…ç³»ç»ŸæŒ‡æ ‡"""
        try:
            if resource_type == "cpu":
                return {"cpu_percent": psutil.cpu_percent(interval=1)}
            elif resource_type == "memory":
                mem = psutil.virtual_memory()
                return {"memory_percent": mem.percent, "memory_used_gb": mem.used / (1024**3)}
            else:
                return {}
        except Exception as e:
            self.logger.warning(f"è·å–{resource_type}æŒ‡æ ‡å¤±è´¥: {e}")
            return {}
    
    async def _test_resource_management_decision(self, test_case: TestCase):
        """æµ‹è¯•èµ„æºç®¡ç†å†³ç­–"""
        message = test_case.input_data["message"]
        max_time = test_case.input_data["max_time"]
        
        result = await self.send_chat_request(message)
        
        if "error" in result:
            self.record_test_result(test_case, TestResult.FAIL, details={"error": result["error"]})
            return
        
        content = result.get("content", "")
        response_time = result.get("_response_time_ms", 0)
        
        # æ£€æŸ¥å†³ç­–å»ºè®®
        expected = test_case.expected_output
        decision_checks = {}
        
        if expected.get("should_suggest_optimization", False):
            optimization_keywords = ["ä¼˜åŒ–", "æ”¹å–„", "æå‡", "å‡å°‘", "é™ä½"]
            decision_checks["suggests_optimization"] = any(kw in content for kw in optimization_keywords)
        
        if expected.get("load_balancing", False):
            balance_keywords = ["å‡è¡¡", "åˆ†é…", "è´Ÿè½½", "å¹³è¡¡"]
            decision_checks["load_balancing"] = any(kw in content for kw in balance_keywords)
        
        decision_quality = sum(1 for v in decision_checks.values() if v is True) / len(decision_checks) if decision_checks else 0
        performance_ok = self.assert_performance(response_time, max_time, test_case.name)
        success = decision_quality >= 0.7 and performance_ok
        
        metrics = TestMetrics(
            response_time_ms=response_time,
            token_count=result.get("token_count", 0),
            model_used=result.get("model_used", ""),
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=success
        )
        
        test_result = TestResult.PASS if success else TestResult.FAIL
        self.record_test_result(test_case, test_result, metrics=metrics,
                              details={"decision_checks": decision_checks, "decision_quality": decision_quality})
    
    async def _test_performance_optimization(self, test_case: TestCase):
        """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½"""
        if "concurrent_requests" in test_case.input_data:
            await self._test_concurrent_handling(test_case)
        else:
            # å•ä¸ªä¼˜åŒ–æµ‹è¯•çš„ç®€åŒ–ç‰ˆæœ¬
            message = test_case.input_data.get("message", "ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½")
            max_time = test_case.input_data["max_time"]
            
            result = await self.send_chat_request(message)
            
            if "error" in result:
                self.record_test_result(test_case, TestResult.FAIL, details={"error": result["error"]})
                return
            
            response_time = result.get("_response_time_ms", 0)
            performance_ok = self.assert_performance(response_time, max_time, test_case.name)
            
            metrics = TestMetrics(
                response_time_ms=response_time,
                token_count=result.get("token_count", 0),
                model_used=result.get("model_used", ""),
                memory_usage_mb=0.0,
                cpu_usage_percent=0.0,
                success=performance_ok
            )
            
            test_result = TestResult.PASS if performance_ok else TestResult.FAIL
            self.record_test_result(test_case, test_result, metrics=metrics)
    
    async def _test_concurrent_handling(self, test_case: TestCase):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚å¤„ç†"""
        concurrent_requests = test_case.input_data["concurrent_requests"]
        max_time = test_case.input_data["max_time"]
        
        # åˆ›å»ºç®€å•æµ‹è¯•è¯·æ±‚
        test_messages = ["ç®€å•é—®é¢˜1", "ç®€å•é—®é¢˜2", "ç®€å•é—®é¢˜3"][:concurrent_requests]
        
        start_time = time.time()
        
        # å¹¶å‘å‘é€è¯·æ±‚
        tasks = [asyncio.create_task(self.send_chat_request(msg)) for msg in test_messages]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_time = int((time.time() - start_time) * 1000)
        
        # åˆ†æç»“æœ
        successful_requests = [r for r in results if isinstance(r, dict) and "error" not in r]
        failed_requests = len(results) - len(successful_requests)
        
        concurrent_analysis = {
            "all_requests_handled": len(successful_requests) == concurrent_requests,
            "no_failures": failed_requests == 0,
            "reasonable_time": total_time <= max_time
        }
        
        success = all(concurrent_analysis.values())
        
        avg_response_time = sum(r.get("_response_time_ms", 0) for r in successful_requests) / len(successful_requests) if successful_requests else 0
        
        metrics = TestMetrics(
            response_time_ms=int(avg_response_time),
            token_count=sum(r.get("token_count", 0) for r in successful_requests),
            model_used="concurrent",
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            success=success
        )
        
        test_result = TestResult.PASS if success else TestResult.FAIL
        self.record_test_result(test_case, test_result, metrics=metrics,
                              details={"concurrent_analysis": concurrent_analysis, "total_time": total_time})
    
    async def run_resource_tests(self):
        """è¿è¡Œæ‰€æœ‰èµ„æºç®¡ç†æµ‹è¯•"""
        self.logger.info("ğŸ”§ å¼€å§‹èµ„æºç®¡ç†åŠŸèƒ½æµ‹è¯•")
        await self.run_all_tests(self.resource_test_cases)


async def main():
    """ä¸»å‡½æ•°"""
    test_suite = ResourceManagementTestSuite()
    
    try:
        await test_suite.run_resource_tests()
    except KeyboardInterrupt:
        test_suite.logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        test_suite.logger.error(f"èµ„æºç®¡ç†æµ‹è¯•è¿è¡Œå¼‚å¸¸: {e}")


if __name__ == "__main__":
    asyncio.run(main())